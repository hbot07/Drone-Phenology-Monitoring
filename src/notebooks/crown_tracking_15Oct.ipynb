{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d917715a",
   "metadata": {},
   "source": [
    "# Tree Crown Tracking (Strict Preset) â€” 15 Oct\n",
    "\n",
    "This notebook bundles a reusable tracker class that links tree crown polygons across consecutive orthomosaics with a strict, high-precision configuration (one-to-one and containment only). It auto-discovers inputs under `input/input_crowns` and `input/input_om`, builds a match graph, and reports quality/complexity metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65c814d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "from dataclasses import dataclass, replace\n",
    "from collections import defaultdict\n",
    "from typing import Any, Dict, List, Optional, Tuple, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import rasterio\n",
    "from rasterio.mask import mask\n",
    "from shapely.geometry import mapping, Polygon\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d142635a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MatchCaseConfig:\n",
    "    name: str\n",
    "    base_similarity_weights: Dict[str, float]\n",
    "    scoring_weights: Dict[str, float]\n",
    "    similarity_threshold: float\n",
    "    min_iou: float = 0.0\n",
    "    min_overlap_prev: float = 0.0\n",
    "    min_overlap_curr: float = 0.0\n",
    "    max_centroid_dist: Optional[float] = None\n",
    "    mutual_best: bool = False\n",
    "    allow_multiple: bool = False\n",
    "    max_edges_per_prev: Optional[int] = None\n",
    "    max_edges_per_curr: Optional[int] = None\n",
    "\n",
    "class TreeTrackingGraph:\n",
    "    \"\"\"\n",
    "    High-precision tree crown tracker across orthomosaics using a directed graph.\n",
    "\n",
    "    - Discovers crown GeoPackages (*.gpkg) and orthomosaics (*.tif)\n",
    "    - Computes per-crown attributes (centroid, area, compactness, eccentricity, etc.)\n",
    "    - Builds a match graph between consecutive OMs using strict cases: one_to_one + containment\n",
    "    - Reports quality metrics and graph complexity\n",
    "\n",
    "    Strict preset parameters (best-known):\n",
    "    - base_max_dist ~ 70-80 (projected units);\n",
    "    - overlap_gate = 0.48; min_base_similarity = 0.35;\n",
    "    - Cases:\n",
    "      one_to_one: similarity_threshold=0.82, min_iou=0.40, min_overlap_prev=0.72, min_overlap_curr=0.72, mutual_best=True, max_edges per node=1\n",
    "      containment: similarity_threshold=0.74, min_overlap_prev=0.82, min_overlap_curr=0.82, max_edges per node=1\n",
    "    \"\"\"\n",
    "    def __init__(self, crown_dir: Optional[str] = None, ortho_dir: Optional[str] = None, output_dir: str = '../../output', simplify_tol: float = 1.0, max_crowns_preview: int = 200, auto_discover: bool = True) -> None:\n",
    "        self.output_dir = output_dir\n",
    "        self.simplify_tol = simplify_tol\n",
    "        self.max_crowns_preview = max_crowns_preview\n",
    "        self.crown_dir = crown_dir or self._resolve_dir('input/input_crowns', '../../input/input_crowns')\n",
    "        self.ortho_dir = ortho_dir or self._resolve_dir('input/input_om', '../../input/input_om')\n",
    "        self.file_pairs: List[Tuple[str, Optional[str]]] = []\n",
    "        self.om_ids: List[int] = []\n",
    "        self.crowns_gdfs: Dict[int, gpd.GeoDataFrame] = {}\n",
    "        self.crown_attrs: Dict[int, List[Dict[str, Any]]] = {}\n",
    "        self.crown_images: Dict[int, List[Optional[np.ndarray]]] = {}\n",
    "        self.G: nx.DiGraph = nx.DiGraph()\n",
    "        self.case_configs: Dict[str, MatchCaseConfig] = self._strict_case_configs()\n",
    "        self.case_order: List[str] = ['one_to_one', 'containment']\n",
    "        self.last_case_counts: Dict[str, int] = {}\n",
    "        self.last_selected_counts: Dict[str, int] = {}\n",
    "        if auto_discover:\n",
    "            self.discover_files()\n",
    "\n",
    "    @staticmethod\n",
    "    def _resolve_dir(root_rel: str, nb_rel: str) -> str:\n",
    "        candidates = [\n",
    "            os.path.abspath(os.path.join(os.getcwd(), root_rel)),\n",
    "            os.path.abspath(os.path.join(os.getcwd(), nb_rel)),\n",
    "        ]\n",
    "        for p in candidates:\n",
    "            if os.path.isdir(p):\n",
    "                return p\n",
    "        raise FileNotFoundError(f\"Could not resolve directory for {root_rel}. Tried: {candidates}\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _extract_numeric_id(name: str) -> Optional[int]:\n",
    "        m = re.search(r\"(\\d+)\", os.path.basename(name))\n",
    "        return int(m.group(1)) if m else None\n",
    "\n",
    "    def discover_files(self) -> None:\n",
    "        crown_files = [os.path.join(self.crown_dir, f) for f in os.listdir(self.crown_dir) if f.lower().endswith('.gpkg')]\n",
    "        ortho_files = [os.path.join(self.ortho_dir, f) for f in os.listdir(self.ortho_dir) if f.lower().endswith('.tif')] if os.path.exists(self.ortho_dir) else []\n",
    "        if not crown_files:\n",
    "            raise FileNotFoundError(f\"No .gpkg crown files found in {self.crown_dir}\")\n",
    "        crowns_by_id = {}\n",
    "        for cf in crown_files:\n",
    "            cid = self._extract_numeric_id(cf)\n",
    "            crowns_by_id[cid if cid is not None else cf] = cf\n",
    "        orthos_by_id = {}\n",
    "        for of in ortho_files:\n",
    "            oid = self._extract_numeric_id(of)\n",
    "            orthos_by_id[oid if oid is not None else of] = of\n",
    "        numeric_ids = sorted(set(k for k in crowns_by_id.keys() if isinstance(k, int)) & set(k for k in orthos_by_id.keys() if isinstance(k, int)))\n",
    "        file_pairs: List[Tuple[str, Optional[str]]] = []\n",
    "        if numeric_ids:\n",
    "            for nid in numeric_ids:\n",
    "                file_pairs.append((crowns_by_id[nid], orthos_by_id.get(nid)))\n",
    "            crown_only = sorted(k for k in crowns_by_id.keys() if isinstance(k, int) and k not in numeric_ids)\n",
    "            for nid in crown_only:\n",
    "                file_pairs.append((crowns_by_id[nid], None))\n",
    "        else:\n",
    "            crown_files_sorted = sorted(crown_files)\n",
    "            ortho_files_sorted = sorted(ortho_files)\n",
    "            for i, cf in enumerate(crown_files_sorted):\n",
    "                of = ortho_files_sorted[i] if i < len(ortho_files_sorted) else None\n",
    "                file_pairs.append((cf, of))\n",
    "        om_ids: List[int] = []\n",
    "        for cf, _ in file_pairs:\n",
    "            cid = self._extract_numeric_id(cf)\n",
    "            om_ids.append(cid if cid is not None else len(om_ids) + 1)\n",
    "        pairs_with_id = sorted([(oid, cf, of) for oid, (cf, of) in zip(om_ids, file_pairs)], key=lambda x: x[0])\n",
    "        self.file_pairs = [(cf, of) for _, cf, of in pairs_with_id]\n",
    "        self.om_ids = [oid for oid, _, _ in pairs_with_id]\n",
    "\n",
    "    def load_data(self, load_images: bool = False) -> None:\n",
    "        self.crowns_gdfs.clear()\n",
    "        self.crown_attrs.clear()\n",
    "        self.crown_images.clear()\n",
    "        for om_id, (crown_file, ortho_file) in zip(self.om_ids, self.file_pairs):\n",
    "            gdf = gpd.read_file(crown_file)\n",
    "            self.crowns_gdfs[om_id] = gdf\n",
    "            self.crown_attrs[om_id] = [self._compute_crown_attributes(row.geometry) for _, row in gdf.iterrows()]\n",
    "            if load_images and ortho_file and os.path.exists(ortho_file):\n",
    "                with rasterio.open(ortho_file) as src:\n",
    "                    patches: List[Optional[np.ndarray]] = []\n",
    "                    for _, row in gdf.iterrows():\n",
    "                        geom = [mapping(row.geometry)]\n",
    "                        try:\n",
    "                            out_image, _ = mask(src, geom, crop=True)\n",
    "                            img_patch = np.moveaxis(out_image, 0, -1)\n",
    "                        except Exception:\n",
    "                            img_patch = None\n",
    "                        patches.append(img_patch)\n",
    "                self.crown_images[om_id] = patches\n",
    "            else:\n",
    "                self.crown_images[om_id] = [None] * len(gdf)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_crown_attributes(geometry) -> Dict[str, Any]:\n",
    "        centroid = geometry.centroid\n",
    "        area = geometry.area\n",
    "        perimeter = geometry.length\n",
    "        bounds = geometry.bounds\n",
    "        compactness = (4 * np.pi * area) / (perimeter ** 2) if perimeter > 0 else 0\n",
    "        try:\n",
    "            min_rect = geometry.minimum_rotated_rectangle\n",
    "            coords = list(min_rect.exterior.coords)\n",
    "            side1 = np.linalg.norm(np.array(coords[0]) - np.array(coords[1]))\n",
    "            side2 = np.linalg.norm(np.array(coords[1]) - np.array(coords[2]))\n",
    "            major_axis = max(side1, side2)\n",
    "            minor_axis = min(side1, side2)\n",
    "            eccentricity = minor_axis / major_axis if major_axis > 0 else 1\n",
    "        except Exception:\n",
    "            eccentricity = 1\n",
    "        aspect_ratio = (bounds[3] - bounds[1]) / (bounds[2] - bounds[0]) if bounds[2] != bounds[0] else 1\n",
    "        return {\n",
    "            'geometry': geometry,\n",
    "            'centroid': centroid,\n",
    "            'area': area,\n",
    "            'perimeter': perimeter,\n",
    "            'compactness': compactness,\n",
    "            'eccentricity': eccentricity,\n",
    "            'aspect_ratio': aspect_ratio,\n",
    "            'bounds': bounds,\n",
    "        }\n",
    "\n",
    "    def _strict_case_configs(self) -> Dict[str, MatchCaseConfig]:\n",
    "        return {\n",
    "            'one_to_one': MatchCaseConfig(\n",
    "                name='one_to_one',\n",
    "                base_similarity_weights={'spatial': 0.45, 'area': 0.2, 'shape': 0.15, 'iou': 0.2},\n",
    "                scoring_weights={'base': 0.55, 'iou': 0.2, 'overlap_prev': 0.1, 'overlap_curr': 0.1, 'centroid': 0.05},\n",
    "                similarity_threshold=0.82,\n",
    "                min_iou=0.40,\n",
    "                min_overlap_prev=0.72,\n",
    "                min_overlap_curr=0.72,\n",
    "                max_centroid_dist=45.0,\n",
    "                mutual_best=True,\n",
    "                allow_multiple=False,\n",
    "                max_edges_per_prev=1,\n",
    "                max_edges_per_curr=1,\n",
    "            ),\n",
    "            'containment': MatchCaseConfig(\n",
    "                name='containment',\n",
    "                base_similarity_weights={'spatial': 0.35, 'area': 0.15, 'shape': 0.15, 'iou': 0.35},\n",
    "                scoring_weights={'base': 0.3, 'overlap_prev': 0.35, 'overlap_curr': 0.35},\n",
    "                similarity_threshold=0.74,\n",
    "                min_iou=0.0,\n",
    "                min_overlap_prev=0.82,\n",
    "                min_overlap_curr=0.82,\n",
    "                max_centroid_dist=60.0,\n",
    "                mutual_best=False,\n",
    "                allow_multiple=False,\n",
    "                max_edges_per_prev=1,\n",
    "                max_edges_per_curr=1,\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_iou(g1, g2) -> float:\n",
    "        try:\n",
    "            intersection = g1.intersection(g2).area\n",
    "            union = g1.union(g2).area\n",
    "        except Exception:\n",
    "            intersection = 0.0\n",
    "            union = g1.area + g2.area\n",
    "        return intersection / union if union > 0 else 0.0\n",
    "\n",
    "    def _weighted_similarity(self, a1: Dict[str, Any], a2: Dict[str, Any], weights: Optional[Dict[str, float]] = None, max_dist: float = 100.0) -> Tuple[float, Dict[str, float]]:\n",
    "        if weights is None:\n",
    "            weights = {'spatial': 0.4, 'area': 0.2, 'shape': 0.2, 'iou': 0.2}\n",
    "        centroid_dist = a1['centroid'].distance(a2['centroid'])\n",
    "        spatial_sim = max(0.0, 1.0 - (centroid_dist / max_dist))\n",
    "        area_sim = min(a1['area'], a2['area']) / max(a1['area'], a2['area']) if max(a1['area'], a2['area']) > 0 else 0.0\n",
    "        compactness_sim = 1.0 - abs(a1['compactness'] - a2['compactness'])\n",
    "        eccentricity_sim = 1.0 - abs(a1['eccentricity'] - a2['eccentricity'])\n",
    "        shape_sim = (compactness_sim + eccentricity_sim) / 2.0\n",
    "        iou_sim = self._compute_iou(a1['geometry'], a2['geometry'])\n",
    "        total = (weights.get('spatial', 0.0) * spatial_sim + weights.get('area', 0.0) * area_sim + weights.get('shape', 0.0) * shape_sim + weights.get('iou', 0.0) * iou_sim)\n",
    "        return total, {'spatial': float(spatial_sim), 'area': float(area_sim), 'shape': float(shape_sim), 'iou': float(iou_sim), 'total': float(total)}\n",
    "\n",
    "    def _compute_pair_metrics(self, prev_attrs: Dict[str, Any], curr_attrs: Dict[str, Any], max_dist: float) -> Dict[str, float]:\n",
    "        prev_geom = prev_attrs['geometry']\n",
    "        curr_geom = curr_attrs['geometry']\n",
    "        try:\n",
    "            intersection_area = prev_geom.intersection(curr_geom).area\n",
    "        except Exception:\n",
    "            intersection_area = 0.0\n",
    "        try:\n",
    "            union_area = prev_geom.union(curr_geom).area\n",
    "        except Exception:\n",
    "            union_area = prev_attrs['area'] + curr_attrs['area'] - intersection_area\n",
    "        prev_area = prev_attrs['area'] if prev_attrs['area'] > 0 else 1e-6\n",
    "        curr_area = curr_attrs['area'] if curr_attrs['area'] > 0 else 1e-6\n",
    "        overlap_prev = intersection_area / prev_area\n",
    "        overlap_curr = intersection_area / curr_area\n",
    "        iou = intersection_area / union_area if union_area > 0 else 0.0\n",
    "        centroid_dist = prev_attrs['centroid'].distance(curr_attrs['centroid'])\n",
    "        base_similarity, parts = self._weighted_similarity(prev_attrs, curr_attrs, max_dist=max_dist)\n",
    "        prev_radius = np.sqrt(prev_area / np.pi)\n",
    "        curr_radius = np.sqrt(curr_area / np.pi)\n",
    "        mean_radius = max((prev_radius + curr_radius) / 2.0, 1e-3)\n",
    "        area_ratio = curr_area / prev_area if prev_area > 0 else np.inf\n",
    "        if not np.isfinite(area_ratio) or area_ratio <= 0:\n",
    "            balanced_area_ratio = 0.0\n",
    "        else:\n",
    "            balanced_area_ratio = area_ratio if area_ratio <= 1 else 1 / area_ratio\n",
    "        try:\n",
    "            prev_contains_curr = prev_geom.buffer(0).contains(curr_geom)\n",
    "        except Exception:\n",
    "            prev_contains_curr = False\n",
    "        try:\n",
    "            curr_contains_prev = curr_geom.buffer(0).contains(prev_geom)\n",
    "        except Exception:\n",
    "            curr_contains_prev = False\n",
    "        return {\n",
    "            'intersection_area': float(intersection_area),\n",
    "            'union_area': float(union_area),\n",
    "            'overlap_prev': float(overlap_prev),\n",
    "            'overlap_curr': float(overlap_curr),\n",
    "            'iou': float(iou),\n",
    "            'centroid_dist': float(centroid_dist),\n",
    "            'base_similarity': float(base_similarity),\n",
    "            'spatial_similarity': float(parts['spatial']),\n",
    "            'area_similarity': float(parts['area']),\n",
    "            'shape_similarity': float(parts['shape']),\n",
    "            'mean_radius': float(mean_radius),\n",
    "            'area_ratio': float(area_ratio if np.isfinite(area_ratio) else 0.0),\n",
    "            'balanced_area_ratio': float(balanced_area_ratio),\n",
    "            'prev_contains_curr': bool(prev_contains_curr),\n",
    "            'curr_contains_prev': bool(curr_contains_prev),\n",
    "        }\n",
    "\n",
    "    def _classify_match_case(self, prev_node: Tuple[int, int], curr_node: Tuple[int, int], features: Dict[str, float], prev_overlap_counts: Dict[Tuple[int, int], int], curr_overlap_counts: Dict[Tuple[int, int], int], overlap_gate: float) -> str:\n",
    "        if features['prev_contains_curr'] or features['curr_contains_prev']:\n",
    "            return 'containment'\n",
    "        # one_to_one: strong mutual overlap & IoU; unique overlaps (strict default).\n",
    "        overlap_prev = features['overlap_prev']\n",
    "        overlap_curr = features['overlap_curr']\n",
    "        iou = features['iou']\n",
    "        prev_count = prev_overlap_counts.get(prev_node, 0)\n",
    "        curr_count = curr_overlap_counts.get(curr_node, 0)\n",
    "        if prev_count == 1 and curr_count == 1 and overlap_prev >= 0.72 and overlap_curr >= 0.72 and iou >= 0.40:\n",
    "            return 'one_to_one'\n",
    "        return 'none'\n",
    "\n",
    "    def _score_candidate(self, base_similarity: float, similarity_parts: Dict[str, float], features: Dict[str, float], config: MatchCaseConfig) -> float:\n",
    "        centroid_factor = 1.0 - min(1.0, features['centroid_dist'] / (features['mean_radius'] * 3.0))\n",
    "        components = {\n",
    "            'base': base_similarity,\n",
    "            'spatial': similarity_parts.get('spatial', 0.0),\n",
    "            'area': similarity_parts.get('area', 0.0),\n",
    "            'shape': similarity_parts.get('shape', 0.0),\n",
    "            'iou': features['iou'],\n",
    "            'overlap_prev': features['overlap_prev'],\n",
    "            'overlap_curr': features['overlap_curr'],\n",
    "            'centroid': max(0.0, centroid_factor),\n",
    "            'area_balance': features.get('balanced_area_ratio', 0.0),\n",
    "        }\n",
    "        score = 0.0\n",
    "        for key, weight in config.scoring_weights.items():\n",
    "            score += weight * components.get(key, 0.0)\n",
    "        return score\n",
    "\n",
    "    def _select_candidates_by_case(self, candidates: List[Dict[str, Any]], configs: Dict[str, MatchCaseConfig], case_order: List[str], max_dist: float) -> List[Dict[str, Any]]:\n",
    "        selected: List[Dict[str, Any]] = []\n",
    "        used_prev: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "        used_curr: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "        for case_name in case_order:\n",
    "            config = configs.get(case_name)\n",
    "            if not config:\n",
    "                continue\n",
    "            case_candidates = [cand for cand in candidates if cand['case'] == case_name]\n",
    "            if not case_candidates:\n",
    "                continue\n",
    "            enriched: List[Dict[str, Any]] = []\n",
    "            for cand in case_candidates:\n",
    "                prev_attrs = cand['prev_attrs']\n",
    "                curr_attrs = cand['curr_attrs']\n",
    "                features = cand['features']\n",
    "                if config.max_centroid_dist is not None and features['centroid_dist'] > config.max_centroid_dist:\n",
    "                    continue\n",
    "                if features['iou'] < config.min_iou:\n",
    "                    continue\n",
    "                if features['overlap_prev'] < config.min_overlap_prev:\n",
    "                    continue\n",
    "                if features['overlap_curr'] < config.min_overlap_curr:\n",
    "                    continue\n",
    "                base_similarity, parts = self._weighted_similarity(prev_attrs, curr_attrs, weights=config.base_similarity_weights, max_dist=max_dist)\n",
    "                score = self._score_candidate(base_similarity, parts, features, config)\n",
    "                if score < config.similarity_threshold:\n",
    "                    continue\n",
    "                cand['base_similarity'] = float(base_similarity)\n",
    "                cand['similarity_parts'] = {k: float(v) for k, v in parts.items()}\n",
    "                cand['score'] = float(score)\n",
    "                enriched.append(cand)\n",
    "            if not enriched:\n",
    "                continue\n",
    "            if config.mutual_best:\n",
    "                best_prev: Dict[Tuple[int, int], Dict[str, Any]] = {}\n",
    "                best_curr: Dict[Tuple[int, int], Dict[str, Any]] = {}\n",
    "                for cand in enriched:\n",
    "                    prev_node = cand['prev_node']\n",
    "                    curr_node = cand['curr_node']\n",
    "                    if used_prev.get(prev_node, 0) and not config.allow_multiple:\n",
    "                        continue\n",
    "                    if used_curr.get(curr_node, 0) and not config.allow_multiple:\n",
    "                        continue\n",
    "                    if cand['score'] < config.similarity_threshold:\n",
    "                        continue\n",
    "                    if prev_node not in best_prev or cand['score'] > best_prev[prev_node]['score']:\n",
    "                        best_prev[prev_node] = cand\n",
    "                    if curr_node not in best_curr or cand['score'] > best_curr[curr_node]['score']:\n",
    "                        best_curr[curr_node] = cand\n",
    "                for cand in enriched:\n",
    "                    prev_node = cand['prev_node']\n",
    "                    curr_node = cand['curr_node']\n",
    "                    if best_prev.get(prev_node) is cand and best_curr.get(curr_node) is cand:\n",
    "                        if not config.allow_multiple:\n",
    "                            if used_prev.get(prev_node, 0) or used_curr.get(curr_node, 0):\n",
    "                                continue\n",
    "                        if config.max_edges_per_prev is not None and used_prev[prev_node] >= config.max_edges_per_prev:\n",
    "                            continue\n",
    "                        if config.max_edges_per_curr is not None and used_curr[curr_node] >= config.max_edges_per_curr:\n",
    "                            continue\n",
    "                        selected.append(cand)\n",
    "                        used_prev[prev_node] += 1\n",
    "                        used_curr[curr_node] += 1\n",
    "            else:\n",
    "                enriched.sort(key=lambda c: c['score'], reverse=True)\n",
    "                for cand in enriched:\n",
    "                    prev_node = cand['prev_node']\n",
    "                    curr_node = cand['curr_node']\n",
    "                    if not config.allow_multiple:\n",
    "                        if used_prev.get(prev_node, 0) or used_curr.get(curr_node, 0):\n",
    "                            continue\n",
    "                    if config.max_edges_per_prev is not None and used_prev[prev_node] >= config.max_edges_per_prev:\n",
    "                        continue\n",
    "                    if config.max_edges_per_curr is not None and used_curr[curr_node] >= config.max_edges_per_curr:\n",
    "                        continue\n",
    "                    selected.append(cand)\n",
    "                    used_prev[prev_node] += 1\n",
    "                    used_curr[curr_node] += 1\n",
    "        return selected\n",
    "\n",
    "    def reset_graph(self) -> None:\n",
    "        self.G = nx.DiGraph()\n",
    "\n",
    "    def build_graph_conditional(self, case_configs: Optional[Dict[str, MatchCaseConfig]] = None, case_order: Optional[List[str]] = None, base_max_dist: float = 75.0, overlap_gate: float = 0.48, min_base_similarity: float = 0.35, max_candidates_per_prev: Optional[int] = None, max_candidates_per_curr: Optional[int] = None) -> None:\n",
    "        if not self.crowns_gdfs:\n",
    "            self.load_data(load_images=False)\n",
    "        self.reset_graph()\n",
    "        configs = {name: replace(cfg) for name, cfg in (case_configs or self.case_configs).items()}\n",
    "        order = case_order or self.case_order\n",
    "        self.last_case_counts = {}\n",
    "        self.last_selected_counts = {name: 0 for name in configs.keys()}\n",
    "        for idx in range(len(self.om_ids)):\n",
    "            om_id = self.om_ids[idx]\n",
    "            gdf = self.crowns_gdfs[om_id]\n",
    "            for crown_id, row in gdf.iterrows():\n",
    "                attrs = self.crown_attrs[om_id][crown_id]\n",
    "                self.G.add_node((om_id, crown_id), **attrs)\n",
    "            if idx == 0:\n",
    "                continue\n",
    "            prev_om = self.om_ids[idx - 1]\n",
    "            prev_nodes = [(prev_om, i) for i in range(len(self.crowns_gdfs[prev_om]))]\n",
    "            curr_nodes = [(om_id, j) for j in range(len(gdf))]\n",
    "            candidates: List[Dict[str, Any]] = []\n",
    "            overlap_counts_prev: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "            overlap_counts_curr: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "            for prev_node in prev_nodes:\n",
    "                prev_attrs = self.G.nodes[prev_node]\n",
    "                for curr_node in curr_nodes:\n",
    "                    curr_attrs = self.crown_attrs[om_id][curr_node[1]]\n",
    "                    features = self._compute_pair_metrics(prev_attrs, curr_attrs, max_dist=base_max_dist)\n",
    "                    if features['centroid_dist'] > base_max_dist:\n",
    "                        continue\n",
    "                    if features['base_similarity'] < min_base_similarity and features['iou'] < overlap_gate:\n",
    "                        continue\n",
    "                    cand = {\n",
    "                        'prev_node': prev_node,\n",
    "                        'curr_node': curr_node,\n",
    "                        'prev_attrs': prev_attrs,\n",
    "                        'curr_attrs': curr_attrs,\n",
    "                        'features': features,\n",
    "                    }\n",
    "                    candidates.append(cand)\n",
    "                    if features['overlap_prev'] >= overlap_gate:\n",
    "                        overlap_counts_prev[prev_node] += 1\n",
    "                    if features['overlap_curr'] >= overlap_gate:\n",
    "                        overlap_counts_curr[curr_node] += 1\n",
    "            if not candidates:\n",
    "                continue\n",
    "            for cand in candidates:\n",
    "                cand['case'] = self._classify_match_case(cand['prev_node'], cand['curr_node'], cand['features'], overlap_counts_prev, overlap_counts_curr, overlap_gate)\n",
    "            candidates = [cand for cand in candidates if cand['case'] != 'none']\n",
    "            if not candidates:\n",
    "                continue\n",
    "            if max_candidates_per_prev is not None:\n",
    "                grouped_prev: Dict[Tuple[int, int], List[Dict[str, Any]]] = defaultdict(list)\n",
    "                for cand in candidates:\n",
    "                    grouped_prev[cand['prev_node']].append(cand)\n",
    "                trimmed: List[Dict[str, Any]] = []\n",
    "                for group in grouped_prev.values():\n",
    "                    group.sort(key=lambda c: (c['features']['base_similarity'], c['features']['iou']), reverse=True)\n",
    "                    trimmed.extend(group[:max_candidates_per_prev])\n",
    "                candidates = trimmed\n",
    "            if max_candidates_per_curr is not None:\n",
    "                grouped_curr: Dict[Tuple[int, int], List[Dict[str, Any]]] = defaultdict(list)\n",
    "                for cand in candidates:\n",
    "                    grouped_curr[cand['curr_node']].append(cand)\n",
    "                trimmed_curr: List[Dict[str, Any]] = []\n",
    "                for group in grouped_curr.values():\n",
    "                    group.sort(key=lambda c: (c['features']['base_similarity'], c['features']['iou']), reverse=True)\n",
    "                    trimmed_curr.extend(group[:max_candidates_per_curr])\n",
    "                candidates = trimmed_curr\n",
    "            case_counts = defaultdict(int)\n",
    "            for cand in candidates:\n",
    "                case_counts[cand['case']] += 1\n",
    "            for case_name, count in case_counts.items():\n",
    "                self.last_case_counts[case_name] = self.last_case_counts.get(case_name, 0) + count\n",
    "            selected = self._select_candidates_by_case(candidates, configs, order, base_max_dist)\n",
    "            for cand in selected:\n",
    "                case_name = cand['case']\n",
    "                features = cand['features']\n",
    "                similarity_parts = cand.get('similarity_parts', {})\n",
    "                self.G.add_edge(cand['prev_node'], cand['curr_node'], similarity=float(cand.get('score', features['base_similarity'])), method='conditional', case=case_name, overlap_prev=float(features['overlap_prev']), overlap_curr=float(features['overlap_curr']), iou=float(features['iou']), centroid_distance=float(features['centroid_dist']), base_similarity=float(cand.get('base_similarity', features['base_similarity'])), spatial_similarity=float(similarity_parts.get('spatial', features['spatial_similarity'])), area_similarity=float(similarity_parts.get('area', features['area_similarity'])), shape_similarity=float(similarity_parts.get('shape', features['shape_similarity'])))\n",
    "                self.last_selected_counts[case_name] = self.last_selected_counts.get(case_name, 0) + 1\n",
    "\n",
    "    def quality_report(self) -> Tuple[str, Dict[str, Any]]:\n",
    "        G = self.G\n",
    "        om_ids = self.om_ids\n",
    "        metrics: Dict[str, Any] = {\n",
    "            'total_trees_detected': G.number_of_nodes(),\n",
    "            'total_edges': G.number_of_edges(),\n",
    "            'total_possible_matches': 0,\n",
    "            'successful_matches': 0,\n",
    "            'match_rate_by_om_pair': {},\n",
    "            'chain_length_distribution': {},\n",
    "            'average_chain_length': 0,\n",
    "            'median_chain_length': 0,\n",
    "            'max_chain_length': 0,\n",
    "        }\n",
    "        chains = self._extract_all_chains()\n",
    "        chain_lengths = [len(chain) for chain in chains]\n",
    "        if chain_lengths:\n",
    "            metrics['average_chain_length'] = float(np.mean(chain_lengths))\n",
    "            metrics['median_chain_length'] = float(np.median(chain_lengths))\n",
    "            metrics['max_chain_length'] = int(max(chain_lengths))\n",
    "            for length in chain_lengths:\n",
    "                metrics['chain_length_distribution'][int(length)] = metrics['chain_length_distribution'].get(int(length), 0) + 1\n",
    "        for i in range(len(om_ids) - 1):\n",
    "            om1, om2 = om_ids[i], om_ids[i + 1]\n",
    "            om1_nodes = [n for n in G.nodes if n[0] == om1]\n",
    "            om2_nodes = [n for n in G.nodes if n[0] == om2]\n",
    "            matches = sum(1 for u, v in G.edges() if u[0] == om1 and v[0] == om2)\n",
    "            possible_matches = min(len(om1_nodes), len(om2_nodes))\n",
    "            match_rate = matches / possible_matches if possible_matches > 0 else 0.0\n",
    "            metrics['match_rate_by_om_pair'][f\"{om1}->{om2}\"] = {\n",
    "                'matches': matches,\n",
    "                'possible': possible_matches,\n",
    "                'rate': float(match_rate),\n",
    "            }\n",
    "            metrics['total_possible_matches'] += possible_matches\n",
    "            metrics['successful_matches'] += matches\n",
    "        metrics['overall_match_rate'] = (metrics['successful_matches'] / metrics['total_possible_matches'] if metrics['total_possible_matches'] > 0 else 0.0)\n",
    "        report = [\n",
    "            \"# Tree Tracking Quality Assessment Report\",\n",
    "            f\"Total Trees Detected: {metrics['total_trees_detected']}\",\n",
    "            f\"Total Tracking Edges: {metrics['total_edges']}\",\n",
    "            f\"Overall Match Rate: {metrics['overall_match_rate']:.3f}\",\n",
    "            f\"Average Chain Length: {metrics.get('average_chain_length', 0):.2f}\",\n",
    "            f\"Maximum Chain Length: {metrics.get('max_chain_length', 0)}\",\n",
    "            \"Match Rates by Orthomosaic Pair:\",\n",
    "        ]\n",
    "        for pair, data in metrics['match_rate_by_om_pair'].items():\n",
    "            report.append(f\"- {pair}: {data['matches']}/{data['possible']} ({data['rate']:.3f})\")\n",
    "        report.append(\"\\nChain Length Distribution:\")\n",
    "        for length, count in sorted(metrics['chain_length_distribution'].items()):\n",
    "            report.append(f\"- Length {length}: {count} trees\")\n",
    "        if self.last_selected_counts:\n",
    "            report.append(\"\\nEdge selection by case:\")\n",
    "            for case_name, count in sorted(self.last_selected_counts.items(), key=lambda kv: (-kv[1], kv[0])):\n",
    "                total_candidates = self.last_case_counts.get(case_name, 0)\n",
    "                if total_candidates:\n",
    "                    ratio = count / total_candidates\n",
    "                    report.append(f\"- {case_name}: {count} / {total_candidates} ({ratio:.2f})\")\n",
    "                else:\n",
    "                    report.append(f\"- {case_name}: {count}\")\n",
    "        return \"\\n\".join(report), metrics\n",
    "\n",
    "    def graph_complexity_metrics(self) -> Dict[str, Any]:\n",
    "        G = self.G\n",
    "        out_deg = dict(G.out_degree())\n",
    "        in_deg = dict(G.in_degree())\n",
    "        def dist(vals: Iterable[int]) -> Dict[int, int]:\n",
    "            hist: Dict[int, int] = {}\n",
    "            for v in vals:\n",
    "                hist[int(v)] = hist.get(int(v), 0) + 1\n",
    "            return dict(sorted(hist.items()))\n",
    "        out_degree_dist = dist(out_deg.values())\n",
    "        in_degree_dist = dist(in_deg.values())\n",
    "        zero_out = sum(1 for v in out_deg.values() if v == 0)\n",
    "        zero_in = sum(1 for v in in_deg.values() if v == 0)\n",
    "        weak_comps = list(nx.weakly_connected_components(G))\n",
    "        strong_comps = list(nx.strongly_connected_components(G))\n",
    "        weak_sizes = sorted([len(c) for c in weak_comps], reverse=True)\n",
    "        strong_sizes = sorted([len(c) for c in strong_comps], reverse=True)\n",
    "        UG = G.to_undirected()\n",
    "        diameters: List[int] = []\n",
    "        for comp in nx.connected_components(UG):\n",
    "            sub = UG.subgraph(comp)\n",
    "            if sub.number_of_nodes() <= 1:\n",
    "                diameters.append(0)\n",
    "            else:\n",
    "                try:\n",
    "                    diameters.append(int(nx.diameter(sub)))\n",
    "                except Exception:\n",
    "                    diameters.append(0)\n",
    "        return {\n",
    "            'num_nodes': G.number_of_nodes(),\n",
    "            'num_edges': G.number_of_edges(),\n",
    "            'avg_out_degree': float(np.mean(list(out_deg.values()))) if out_deg else 0.0,\n",
    "            'avg_in_degree': float(np.mean(list(in_deg.values()))) if in_deg else 0.0,\n",
    "            'out_degree_distribution': out_degree_dist,\n",
    "            'in_degree_distribution': in_degree_dist,\n",
    "            'zero_out_degree_nodes': zero_out,\n",
    "            'zero_in_degree_nodes': zero_in,\n",
    "            'weak_components': len(weak_comps),\n",
    "            'weak_component_sizes': weak_sizes,\n",
    "            'strong_components': len(strong_comps),\n",
    "            'strong_component_sizes': strong_sizes,\n",
    "            'diameters': diameters,\n",
    "            'average_diameter': float(np.mean(diameters)) if diameters else 0.0,\n",
    "            'median_diameter': float(np.median(diameters)) if diameters else 0.0,\n",
    "            'max_diameter': int(max(diameters)) if diameters else 0,\n",
    "        }\n",
    "\n",
    "    def complexity_report(self) -> Tuple[str, Dict[str, Any]]:\n",
    "        m = self.graph_complexity_metrics()\n",
    "        report = [\n",
    "            \"# Graph Complexity Report\",\n",
    "            f\"Nodes: {m['num_nodes']}\",\n",
    "            f\"Edges: {m['num_edges']}\",\n",
    "            f\"Avg out-degree: {m['avg_out_degree']:.3f}\",\n",
    "            f\"Avg in-degree: {m['avg_in_degree']:.3f}\",\n",
    "            f\"Zero out-degree nodes: {m['zero_out_degree_nodes']}\",\n",
    "            f\"Zero in-degree nodes: {m['zero_in_degree_nodes']}\",\n",
    "            f\"Weakly connected components: {m['weak_components']} (sizes head: {m['weak_component_sizes'][:10]})\",\n",
    "            f\"Strongly connected components: {m['strong_components']} (sizes head: {m['strong_component_sizes'][:10]})\",\n",
    "            f\"Average diameter: {m['average_diameter']:.3f}\",\n",
    "            f\"Median diameter: {m['median_diameter']:.3f}\",\n",
    "            f\"Max diameter: {m['max_diameter']}\",\n",
    "        ]\n",
    "        return \"\\n\".join(report), m\n",
    "\n",
    "    def _extract_all_chains(self) -> List[List[Tuple[int, int]]]:\n",
    "        visited = set()\n",
    "        chains: List[List[Tuple[int, int]]] = []\n",
    "        chain_starts = [n for n in self.G.nodes if not list(self.G.predecessors(n))]\n",
    "        for start_node in chain_starts:\n",
    "            if start_node in visited:\n",
    "                continue\n",
    "            chain = self._greedy_chain(start_node)\n",
    "            chains.append(chain)\n",
    "            visited.update(chain)\n",
    "        remaining = set(self.G.nodes) - visited\n",
    "        for node in remaining:\n",
    "            chains.append([node])\n",
    "        return chains\n",
    "\n",
    "    def _greedy_chain(self, start_node: Tuple[int, int]) -> List[Tuple[int, int]]:\n",
    "        chain = [start_node]\n",
    "        current = start_node\n",
    "        while True:\n",
    "            successors = list(self.G.successors(current))\n",
    "            if not successors:\n",
    "                break\n",
    "            if len(successors) > 1:\n",
    "                best_successor = max(successors, key=lambda n: self.G[current][n].get('similarity', 0.0))\n",
    "                chain.append(best_successor)\n",
    "                current = best_successor\n",
    "            else:\n",
    "                chain.append(successors[0])\n",
    "                current = successors[0]\n",
    "        return chain\n",
    "\n",
    "    def get_matching_chain(self, start_om_id: int, crown_id: int) -> List[Tuple[int, int]]:\n",
    "        node = (start_om_id, crown_id)\n",
    "        if node not in self.G:\n",
    "            raise ValueError(f\"Node {(start_om_id, crown_id)} not in graph. Build the graph first.\")\n",
    "        return self._greedy_chain(node)\n",
    "\n",
    "    def ensure_output_dir(self) -> None:\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    def save_text(self, text: str, filename: str) -> str:\n",
    "        self.ensure_output_dir()\n",
    "        path = os.path.join(self.output_dir, filename)\n",
    "        with open(path, 'w') as f:\n",
    "            f.write(text)\n",
    "        return path\n",
    "\n",
    "    def save_json(self, data: Dict[str, Any], filename: str) -> str:\n",
    "        self.ensure_output_dir()\n",
    "        path = os.path.join(self.output_dir, filename)\n",
    "        with open(path, 'w') as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        return path\n",
    "\n",
    "    def run_strict_preset(self, *, base_max_dist: float = 75.0, overlap_gate: float = 0.48, min_base_similarity: float = 0.35, save_prefix: Optional[str] = None) -> Dict[str, Any]:\n",
    "        if not self.crowns_gdfs:\n",
    "            self.load_data(load_images=False)\n",
    "        self.build_graph_conditional(case_configs=self.case_configs, case_order=self.case_order, base_max_dist=base_max_dist, overlap_gate=overlap_gate, min_base_similarity=min_base_similarity)\n",
    "        q_report, q_metrics = self.quality_report()\n",
    "        c_report, c_metrics = self.complexity_report()\n",
    "        artifacts = {\n",
    "            'quality_report_path': None,\n",
    "            'quality_metrics_path': None,\n",
    "            'complexity_report_path': None,\n",
    "            'complexity_metrics_path': None,\n",
    "        }\n",
    "        if save_prefix:\n",
    "            artifacts['quality_report_path'] = self.save_text(q_report, f'{save_prefix}_quality_report.txt')\n",
    "            artifacts['quality_metrics_path'] = self.save_json(q_metrics, f'{save_prefix}_quality_metrics.json')\n",
    "            artifacts['complexity_report_path'] = self.save_text(c_report, f'{save_prefix}_complexity_report.txt')\n",
    "            artifacts['complexity_metrics_path'] = self.save_json(c_metrics, f'{save_prefix}_complexity_metrics.json')\n",
    "        return {\n",
    "            'nodes': self.G.number_of_nodes(),\n",
    "            'edges': self.G.number_of_edges(),\n",
    "            'quality_report': q_report,\n",
    "            'quality_metrics': q_metrics,\n",
    "            'complexity_report': c_report,\n",
    "            'complexity_metrics': c_metrics,\n",
    "            **artifacts,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aba96055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: 626, Edges: 2\n",
      "# Tree Tracking Quality Assessment Report\n",
      "Total Trees Detected: 626\n",
      "Total Tracking Edges: 2\n",
      "Overall Match Rate: 0.004\n",
      "Average Chain Length: 1.00\n",
      "Maximum Chain Length: 2\n",
      "Match Rates by Orthomosaic Pair:\n",
      "- 1->2: 1/80 (0.013)\n",
      "- 2->3: 0/116 (0.000)\n",
      "- 3->4: 0/130 (0.000)\n",
      "- 4->5: 1/150 (0.007)\n",
      "\n",
      "Chain Length Distribution:\n",
      "- Length 1: 622 trees\n",
      "- Length 2: 2 trees\n",
      "\n",
      "Edge selection by case:\n",
      "- one_to_one: 2 / 2 (1.00)\n",
      "- containment: 0 / 3 (0.00)\n",
      "---\n",
      "# Graph Complexity Report\n",
      "Nodes: 626\n",
      "Edges: 2\n",
      "Avg out-degree: 0.003\n",
      "Avg in-degree: 0.003\n",
      "Zero out-degree nodes: 624\n",
      "Zero in-degree nodes: 624\n",
      "Weakly connected components: 624 (sizes head: [2, 2, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Strongly connected components: 626 (sizes head: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Average diameter: 0.003\n",
      "Median diameter: 0.000\n",
      "Max diameter: 1\n"
     ]
    }
   ],
   "source": [
    "# Configure (optional): override default input directories if needed\n",
    "# tracker = TreeTrackingGraph(crown_dir='../../input/input_crowns', ortho_dir='../../input/input_om')\n",
    "tracker = TreeTrackingGraph()\n",
    "summary = tracker.run_strict_preset(base_max_dist=75.0, overlap_gate=0.48, min_base_similarity=0.35, save_prefix='strict_15oct')\n",
    "print(f\"Nodes: {summary['nodes']}, Edges: {summary['edges']}\")\n",
    "print(summary['quality_report'])\n",
    "print('---')\n",
    "print(summary['complexity_report'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "095775cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch quality_report to avoid f-string quote conflicts\n",
    "from typing import Iterable, Tuple as _Tuple\n",
    "def _quality_report_patch(self) -> _Tuple[str, Dict[str, Any]]:\n",
    "    G = self.G\n",
    "    om_ids = self.om_ids\n",
    "    metrics: Dict[str, Any] = {\n",
    "        'total_trees_detected': G.number_of_nodes(),\n",
    "        'total_edges': G.number_of_edges(),\n",
    "        'total_possible_matches': 0,\n",
    "        'successful_matches': 0,\n",
    "        'match_rate_by_om_pair': {},\n",
    "        'chain_length_distribution': {},\n",
    "        'average_chain_length': 0,\n",
    "        'median_chain_length': 0,\n",
    "        'max_chain_length': 0,\n",
    "    }\n",
    "    chains = self._extract_all_chains()\n",
    "    chain_lengths = [len(chain) for chain in chains]\n",
    "    if chain_lengths:\n",
    "        metrics['average_chain_length'] = float(np.mean(chain_lengths))\n",
    "        metrics['median_chain_length'] = float(np.median(chain_lengths))\n",
    "        metrics['max_chain_length'] = int(max(chain_lengths))\n",
    "        for length in chain_lengths:\n",
    "            key = int(length)\n",
    "            metrics['chain_length_distribution'][key] = metrics['chain_length_distribution'].get(key, 0) + 1\n",
    "    for i in range(len(om_ids) - 1):\n",
    "        om1, om2 = om_ids[i], om_ids[i + 1]\n",
    "        om1_nodes = [n for n in G.nodes if n[0] == om1]\n",
    "        om2_nodes = [n for n in G.nodes if n[0] == om2]\n",
    "        matches = sum(1 for u, v in G.edges() if u[0] == om1 and v[0] == om2)\n",
    "        possible_matches = min(len(om1_nodes), len(om2_nodes))\n",
    "        rate = matches / possible_matches if possible_matches > 0 else 0.0\n",
    "        metrics['match_rate_by_om_pair'][f\"{om1}->{om2}\"] = {\n",
    "            'matches': matches,\n",
    "            'possible': possible_matches,\n",
    "            'rate': float(rate),\n",
    "        }\n",
    "        metrics['total_possible_matches'] += possible_matches\n",
    "        metrics['successful_matches'] += matches\n",
    "    metrics['overall_match_rate'] = (metrics['successful_matches'] / metrics['total_possible_matches'] if metrics['total_possible_matches'] > 0 else 0.0)\n",
    "    report = []\n",
    "    report.append('# Tree Tracking Quality Assessment Report')\n",
    "    report.append(f\"Total Trees Detected: {metrics['total_trees_detected']}\")\n",
    "    report.append(f\"Total Tracking Edges: {metrics['total_edges']}\")\n",
    "    report.append(f\"Overall Match Rate: {metrics['overall_match_rate']:.3f}\")\n",
    "    report.append(f\"Average Chain Length: {metrics.get('average_chain_length', 0):.2f}\")\n",
    "    report.append(f\"Maximum Chain Length: {metrics.get('max_chain_length', 0)}\")\n",
    "    report.append('Match Rates by Orthomosaic Pair:')\n",
    "    for pair, data in metrics['match_rate_by_om_pair'].items():\n",
    "        report.append(f\"- {pair}: {data['matches']}/{data['possible']} ({data['rate']:.3f})\")\n",
    "    report.append('\\nChain Length Distribution:')\n",
    "    for length, count in sorted(metrics['chain_length_distribution'].items()):\n",
    "        report.append(f\"- Length {length}: {count} trees\")\n",
    "    if self.last_selected_counts:\n",
    "        report.append('\\nEdge selection by case:')\n",
    "        for case_name, count in sorted(self.last_selected_counts.items(), key=lambda kv: (-kv[1], kv[0])):\n",
    "            total_candidates = self.last_case_counts.get(case_name, 0)\n",
    "            if total_candidates:\n",
    "                ratio = count / total_candidates\n",
    "                report.append(f\"- {case_name}: {count} / {total_candidates} ({ratio:.2f})\")\n",
    "            else:\n",
    "                report.append(f\"- {case_name}: {count}\")\n",
    "    return '\\n'.join(report), metrics\n",
    "TreeTrackingGraph.quality_report = _quality_report_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b8c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: list a few high-confidence chains (strict edges tend to be high-confidence)\n",
    "def _edge_info(tracker: TreeTrackingGraph, chain: List[Tuple[int, int]]):\n",
    "    for u, v in zip(chain, chain[1:]):\n",
    "        data = tracker.G.get_edge_data(u, v) or {}\n",
    "        yield u, v, data\n",
    "\n",
    "def filter_high_confidence_chains(tracker: TreeTrackingGraph, min_similarity: float = 0.8, allowed_cases: Optional[set] = None):\n",
    "    if allowed_cases is None:\n",
    "        allowed_cases = {'one_to_one', 'containment'}\n",
    "    chains = tracker._extract_all_chains()\n",
    "    high_conf = []\n",
    "    for chain in chains:\n",
    "        if len(chain) < 2:\n",
    "            continue\n",
    "        edges = list(_edge_info(tracker, chain))\n",
    "        if not edges:\n",
    "            continue\n",
    "        if all((e.get('case') in allowed_cases) and (e.get('similarity', 0.0) >= min_similarity) for _, _, e in edges):\n",
    "            high_conf.append(chain)\n",
    "    return high_conf\n",
    "\n",
    "hc = filter_high_confidence_chains(tracker, min_similarity=0.8)\n",
    "print(f\"High-confidence chains found: {len(hc)}\")\n",
    "for idx, ch in enumerate(hc[:10], start=1):\n",
    "    sims = [tracker.G.get_edge_data(u, v).get('similarity', 0.0) for u, v in zip(ch, ch[1:])]\n",
    "    print(f\"{idx:02d}. length={len(ch)} avg_sim={np.mean(sims) if sims else 0:.3f} nodes={ch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef6d0f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extended matching strategies: shuffled temporal orders and virtual all-pairs edges\n",
    "import random\n",
    "from typing import Set\n",
    "\n",
    "def build_graph_conditional_for_order(self, om_sequence, base_max_dist: float = 75.0, overlap_gate: float = 0.48, min_base_similarity: float = 0.35, max_candidates_per_prev: Optional[int] = None, max_candidates_per_curr: Optional[int] = None, case_configs: Optional[Dict[str, MatchCaseConfig]] = None, case_order: Optional[List[str]] = None) -> None:\n",
    "    if not self.crowns_gdfs:\n",
    "        self.load_data(load_images=False)\n",
    "    self.reset_graph()\n",
    "    configs = {name: replace(cfg) for name, cfg in (case_configs or self.case_configs).items()}\n",
    "    order = case_order or self.case_order\n",
    "    self.last_case_counts = {}\n",
    "    self.last_selected_counts = {name: 0 for name in configs.keys()}\n",
    "    # Add nodes once\n",
    "    for om_id in om_sequence:\n",
    "        gdf = self.crowns_gdfs[om_id]\n",
    "        for crown_id, row in gdf.iterrows():\n",
    "            attrs = self.crown_attrs[om_id][crown_id]\n",
    "            self.G.add_node((om_id, crown_id), **attrs)\n",
    "    # Edges for consecutive pairs in the provided order\n",
    "    for idx in range(1, len(om_sequence)):\n",
    "        prev_om, om_id = om_sequence[idx - 1], om_sequence[idx]\n",
    "        gdf = self.crowns_gdfs[om_id]\n",
    "        prev_nodes = [(prev_om, i) for i in range(len(self.crowns_gdfs[prev_om]))]\n",
    "        curr_nodes = [(om_id, j) for j in range(len(gdf))]\n",
    "        candidates: List[Dict[str, Any]] = []\n",
    "        overlap_counts_prev: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "        overlap_counts_curr: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "        for prev_node in prev_nodes:\n",
    "            prev_attrs = self.G.nodes[prev_node]\n",
    "            for curr_node in curr_nodes:\n",
    "                curr_attrs = self.crown_attrs[om_id][curr_node[1]]\n",
    "                features = self._compute_pair_metrics(prev_attrs, curr_attrs, max_dist=base_max_dist)\n",
    "                if features['centroid_dist'] > base_max_dist:\n",
    "                    continue\n",
    "                if features['base_similarity'] < min_base_similarity and features['iou'] < overlap_gate:\n",
    "                    continue\n",
    "                cand = {\n",
    "                    'prev_node': prev_node,\n",
    "                    'curr_node': curr_node,\n",
    "                    'prev_attrs': prev_attrs,\n",
    "                    'curr_attrs': curr_attrs,\n",
    "                    'features': features,\n",
    "                }\n",
    "                candidates.append(cand)\n",
    "                if features['overlap_prev'] >= overlap_gate:\n",
    "                    overlap_counts_prev[prev_node] += 1\n",
    "                if features['overlap_curr'] >= overlap_gate:\n",
    "                    overlap_counts_curr[curr_node] += 1\n",
    "        if not candidates:\n",
    "            continue\n",
    "        for cand in candidates:\n",
    "            cand['case'] = self._classify_match_case(cand['prev_node'], cand['curr_node'], cand['features'], overlap_counts_prev, overlap_counts_curr, overlap_gate)\n",
    "        candidates = [cand for cand in candidates if cand['case'] != 'none']\n",
    "        if not candidates:\n",
    "            continue\n",
    "        if max_candidates_per_prev is not None:\n",
    "            grouped_prev: Dict[Tuple[int, int], List[Dict[str, Any]]] = defaultdict(list)\n",
    "            for cand in candidates:\n",
    "                grouped_prev[cand['prev_node']].append(cand)\n",
    "            trimmed: List[Dict[str, Any]] = []\n",
    "            for group in grouped_prev.values():\n",
    "                group.sort(key=lambda c: (c['features']['base_similarity'], c['features']['iou']), reverse=True)\n",
    "                trimmed.extend(group[:max_candidates_per_prev])\n",
    "            candidates = trimmed\n",
    "        if max_candidates_per_curr is not None:\n",
    "            grouped_curr: Dict[Tuple[int, int], List[Dict[str, Any]]] = defaultdict(list)\n",
    "            for cand in candidates:\n",
    "                grouped_curr[cand['curr_node']].append(cand)\n",
    "            trimmed_curr: List[Dict[str, Any]] = []\n",
    "            for group in grouped_curr.values():\n",
    "                group.sort(key=lambda c: (c['features']['base_similarity'], c['features']['iou']), reverse=True)\n",
    "                trimmed_curr.extend(group[:max_candidates_per_curr])\n",
    "            candidates = trimmed_curr\n",
    "        case_counts = defaultdict(int)\n",
    "        for cand in candidates:\n",
    "            case_counts[cand['case']] += 1\n",
    "        for case_name, count in case_counts.items():\n",
    "            self.last_case_counts[case_name] = self.last_case_counts.get(case_name, 0) + count\n",
    "        selected = self._select_candidates_by_case(candidates, configs, order, base_max_dist)\n",
    "        for cand in selected:\n",
    "            case_name = cand['case']\n",
    "            features = cand['features']\n",
    "            similarity_parts = cand.get('similarity_parts', {})\n",
    "            self.G.add_edge(cand['prev_node'], cand['curr_node'], similarity=float(cand.get('score', features['base_similarity'])), method='conditional_order', case=case_name, overlap_prev=float(features['overlap_prev']), overlap_curr=float(features['overlap_curr']), iou=float(features['iou']), centroid_distance=float(features['centroid_dist']), base_similarity=float(cand.get('base_similarity', features['base_similarity'])), spatial_similarity=float(similarity_parts.get('spatial', features['spatial_similarity'])), area_similarity=float(similarity_parts.get('area', features['area_similarity'])), shape_similarity=float(similarity_parts.get('shape', features['shape_similarity'])))\n",
    "            self.last_selected_counts[case_name] = self.last_selected_counts.get(case_name, 0) + 1\n",
    "\n",
    "def build_graph_from_shuffles(self, num_shuffles: int = 8, seed: Optional[int] = 42, base_max_dist: float = 75.0, overlap_gate: float = 0.48, min_base_similarity: float = 0.35, min_frequency: float = 0.5) -> None:\n",
    "    \"\"\"Aggregate edges over random orderings; keep edges appearing in >= min_frequency * num_shuffles.\"\"\"\n",
    "    if not self.crowns_gdfs:\n",
    "        self.load_data(load_images=False)\n",
    "    rng = random.Random(seed)\n",
    "    edge_aggr: Dict[Tuple[Tuple[int,int], Tuple[int,int]], Dict[str, Any]] = {}\n",
    "    # Run shuffled orders and aggregate\n",
    "    for s in range(num_shuffles):\n",
    "        order = list(self.om_ids)\n",
    "        rng.shuffle(order)\n",
    "        temp = TreeTrackingGraph(auto_discover=False)\n",
    "        # shallow copy data\n",
    "        temp.crown_dir = self.crown_dir\n",
    "        temp.ortho_dir = self.ortho_dir\n",
    "        temp.file_pairs = list(self.file_pairs)\n",
    "        temp.om_ids = list(self.om_ids)\n",
    "        temp.crowns_gdfs = self.crowns_gdfs\n",
    "        temp.crown_attrs = self.crown_attrs\n",
    "        temp.crown_images = self.crown_images\n",
    "        temp.case_configs = {name: replace(cfg) for name, cfg in self.case_configs.items()}\n",
    "        temp.case_order = list(self.case_order)\n",
    "        temp.build_graph_conditional_for_order(order, base_max_dist=base_max_dist, overlap_gate=overlap_gate, min_base_similarity=min_base_similarity)\n",
    "        for u, v, data in temp.G.edges(data=True):\n",
    "            key = (u, v)\n",
    "            rec = edge_aggr.get(key)\n",
    "            if rec is None:\n",
    "                rec = {'count': 0, 'sim_sum': 0.0, 'last': data}\n",
    "                edge_aggr[key] = rec\n",
    "            rec['count'] += 1\n",
    "            rec['sim_sum'] += float(data.get('similarity', 0.0))\n",
    "    # Build final aggregated graph\n",
    "    self.reset_graph()\n",
    "    # add nodes\n",
    "    for om_id in self.om_ids:\n",
    "        gdf = self.crowns_gdfs[om_id]\n",
    "        for crown_id, row in gdf.iterrows():\n",
    "            self.G.add_node((om_id, crown_id), **self.crown_attrs[om_id][crown_id])\n",
    "    keep_min_count = max(1, int(np.ceil(min_frequency * num_shuffles)))\n",
    "    for (u, v), rec in edge_aggr.items():\n",
    "        if rec['count'] >= keep_min_count:\n",
    "            avg_sim = rec['sim_sum'] / rec['count'] if rec['count'] else 0.0\n",
    "            self.G.add_edge(u, v, similarity=float(avg_sim), agg_count=int(rec['count']), agg_sim_sum=float(rec['sim_sum']), method='shuffle_agg', case=rec['last'].get('case', 'one_to_one'))\n",
    "\n",
    "def build_graph_virtual_allpairs(self, base_max_dist: float = 75.0, overlap_gate: float = 0.48, min_base_similarity: float = 0.35) -> None:\n",
    "    \"\"\"Allow edges between any pair of OMs (i < j). Enforce strict cases and global 1-1 via used_prev/used_curr.\"\"\"\n",
    "    if not self.crowns_gdfs:\n",
    "        self.load_data(load_images=False)\n",
    "    self.reset_graph()\n",
    "    configs = {name: replace(cfg) for name, cfg in self.case_configs.items()}\n",
    "    order = self.case_order\n",
    "    # Add nodes\n",
    "    for om_id in self.om_ids:\n",
    "        gdf = self.crowns_gdfs[om_id]\n",
    "        for crown_id, row in gdf.iterrows():\n",
    "            attrs = self.crown_attrs[om_id][crown_id]\n",
    "            self.G.add_node((om_id, crown_id), **attrs)\n",
    "    self.last_case_counts = {}\n",
    "    self.last_selected_counts = {name: 0 for name in configs.keys()}\n",
    "    used_prev: Dict[Tuple[int,int], int] = defaultdict(int)\n",
    "    used_curr: Dict[Tuple[int,int], int] = defaultdict(int)\n",
    "    for i_idx in range(len(self.om_ids)):\n",
    "        for j_idx in range(i_idx + 1, len(self.om_ids)):\n",
    "            prev_om = self.om_ids[i_idx]\n",
    "            om_id = self.om_ids[j_idx]\n",
    "            gdf = self.crowns_gdfs[om_id]\n",
    "            prev_nodes = [(prev_om, i) for i in range(len(self.crowns_gdfs[prev_om]))]\n",
    "            curr_nodes = [(om_id, j) for j in range(len(gdf))]\n",
    "            candidates: List[Dict[str, Any]] = []\n",
    "            overlap_counts_prev: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "            overlap_counts_curr: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "            for prev_node in prev_nodes:\n",
    "                prev_attrs = self.G.nodes[prev_node]\n",
    "                for curr_node in curr_nodes:\n",
    "                    curr_attrs = self.crown_attrs[om_id][curr_node[1]]\n",
    "                    features = self._compute_pair_metrics(prev_attrs, curr_attrs, max_dist=base_max_dist)\n",
    "                    if features['centroid_dist'] > base_max_dist:\n",
    "                        continue\n",
    "                    if features['base_similarity'] < min_base_similarity and features['iou'] < overlap_gate:\n",
    "                        continue\n",
    "                    cand = {'prev_node': prev_node, 'curr_node': curr_node, 'prev_attrs': prev_attrs, 'curr_attrs': curr_attrs, 'features': features}\n",
    "                    candidates.append(cand)\n",
    "                    if features['overlap_prev'] >= overlap_gate:\n",
    "                        overlap_counts_prev[prev_node] += 1\n",
    "                    if features['overlap_curr'] >= overlap_gate:\n",
    "                        overlap_counts_curr[curr_node] += 1\n",
    "            if not candidates:\n",
    "                continue\n",
    "            for cand in candidates:\n",
    "                cand['case'] = self._classify_match_case(cand['prev_node'], cand['curr_node'], cand['features'], overlap_counts_prev, overlap_counts_curr, overlap_gate)\n",
    "            candidates = [cand for cand in candidates if cand['case'] != 'none']\n",
    "            if not candidates:\n",
    "                continue\n",
    "            # Select per pair\n",
    "            selected = self._select_candidates_by_case(candidates, configs, order, base_max_dist)\n",
    "            # Apply global 1:1 caps\n",
    "            for cand in selected:\n",
    "                u = cand['prev_node']; v = cand['curr_node']\n",
    "                if used_prev.get(u, 0) and not configs[cand['case']].allow_multiple:\n",
    "                    continue\n",
    "                if used_curr.get(v, 0) and not configs[cand['case']].allow_multiple:\n",
    "                    continue\n",
    "                features = cand['features']\n",
    "                similarity_parts = cand.get('similarity_parts', {})\n",
    "                self.G.add_edge(u, v, similarity=float(cand.get('score', features['base_similarity'])), method='virtual_allpairs', case=cand['case'], overlap_prev=float(features['overlap_prev']), overlap_curr=float(features['overlap_curr']), iou=float(features['iou']), centroid_distance=float(features['centroid_dist']), base_similarity=float(cand.get('base_similarity', features['base_similarity'])), spatial_similarity=float(similarity_parts.get('spatial', features['spatial_similarity'])), area_similarity=float(similarity_parts.get('area', features['area_similarity'])), shape_similarity=float(similarity_parts.get('shape', features['shape_similarity'])))\n",
    "                used_prev[u] += 1\n",
    "                used_curr[v] += 1\n",
    "                self.last_selected_counts[cand['case']] = self.last_selected_counts.get(cand['case'], 0) + 1\n",
    "\n",
    "# Bind the methods to the class\n",
    "TreeTrackingGraph.build_graph_conditional_for_order = build_graph_conditional_for_order\n",
    "TreeTrackingGraph.build_graph_from_shuffles = build_graph_from_shuffles\n",
    "TreeTrackingGraph.build_graph_virtual_allpairs = build_graph_virtual_allpairs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab81aac8",
   "metadata": {},
   "source": [
    "## Virtual edges (enhanced): algorithm overview\n",
    "\n",
    "This variant considers edges between any two orthomosaics i < j, not just consecutive pairs.\n",
    "We score each candidate crown pair with the strict case-based similarity and multiply by a gap penalty so that long jumps are discouraged.\n",
    "\n",
    "Key ideas:\n",
    "- Candidate generation: for every pair of OMs (i, j), compute features (IoU, overlaps, centroid distance) and a base similarity using geometry-derived parts.\n",
    "- Case classification: only two strict cases are allowed â€” one_to_one and containment â€” using strong overlap/IoU gates.\n",
    "- Scoring: case-specific weighted score, then apply a gap weight w_gap = exp(-alpha * (gap - 1)).\n",
    "- Global selection: sort all candidates across all (i, j) by score * w_gap and greedily select edges with at most one outgoing per node and at most one incoming per node (maintains chains).\n",
    "- Output: edges annotated with score, base similarity, IoU/overlaps/centroid distance, case, gap, and gap_weight.\n",
    "\n",
    "Benefits:\n",
    "- Resilient to missing detections in intermediate OMs (can bridge gaps), while still preferring short, consistent links.\n",
    "- Preserves strictness (1 in, 1 out per node) to avoid merges/splits and keep interpretable tracks.\n",
    "\n",
    "Parameters to tune:\n",
    "- alpha (gap decay): higher alpha penalizes long jumps more strongly.\n",
    "- base_max_dist, overlap_gate, min_base_similarity: spatial/similarity gates to form plausible candidates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fc974c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "def build_graph_virtual_allpairs_enhanced(self, base_max_dist: float = 75.0, overlap_gate: float = 0.48, min_base_similarity: float = 0.35, alpha: float = 0.9) -> None:\n",
    "    \"\"\"\n",
    "    Enhanced virtual-all-pairs with gap penalty and global greedy 1:1 selection.\n",
    "    - Generate candidates for all i<j OM pairs that pass strict gates.\n",
    "    - Score with case-specific weights and multiply by exp(-alpha * (gap-1)).\n",
    "    - Sort candidates by final score and greedily select, enforcing <=1 out, <=1 in per node.\n",
    "    \"\"\"\n",
    "    if not self.crowns_gdfs:\n",
    "        self.load_data(load_images=False)\n",
    "    self.reset_graph()\n",
    "    configs = {name: replace(cfg) for name, cfg in self.case_configs.items()}\n",
    "    order = self.case_order\n",
    "    # Add nodes\n",
    "    for om_id in self.om_ids:\n",
    "        gdf = self.crowns_gdfs[om_id]\n",
    "        for crown_id, row in gdf.iterrows():\n",
    "            attrs = self.crown_attrs[om_id][crown_id]\n",
    "            self.G.add_node((om_id, crown_id), **attrs)\n",
    "    self.last_case_counts = {}\n",
    "    self.last_selected_counts = {name: 0 for name in configs.keys()}\n",
    "\n",
    "    all_candidates: List[Dict[str, Any]] = []\n",
    "    # Collect candidates across all i<j\n",
    "    for i_idx in range(len(self.om_ids)):\n",
    "        for j_idx in range(i_idx + 1, len(self.om_ids)):\n",
    "            prev_om = self.om_ids[i_idx]\n",
    "            om_id = self.om_ids[j_idx]\n",
    "            gap = j_idx - i_idx\n",
    "            prev_nodes = [(prev_om, i) for i in range(len(self.crowns_gdfs[prev_om]))]\n",
    "            curr_nodes = [(om_id, j) for j in range(len(self.crowns_gdfs[om_id]))]\n",
    "            overlap_counts_prev: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "            overlap_counts_curr: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "            prelim: List[Dict[str, Any]] = []\n",
    "            for prev_node in prev_nodes:\n",
    "                prev_attrs = self.G.nodes[prev_node]\n",
    "                for curr_node in curr_nodes:\n",
    "                    curr_attrs = self.crown_attrs[om_id][curr_node[1]]\n",
    "                    features = self._compute_pair_metrics(prev_attrs, curr_attrs, max_dist=base_max_dist)\n",
    "                    if features['centroid_dist'] > base_max_dist:\n",
    "                        continue\n",
    "                    if features['base_similarity'] < min_base_similarity and features['iou'] < overlap_gate:\n",
    "                        continue\n",
    "                    rec = {'prev_node': prev_node, 'curr_node': curr_node, 'prev_attrs': prev_attrs, 'curr_attrs': curr_attrs, 'features': features, 'gap': gap}\n",
    "                    prelim.append(rec)\n",
    "                    if features['overlap_prev'] >= overlap_gate:\n",
    "                        overlap_counts_prev[prev_node] += 1\n",
    "                    if features['overlap_curr'] >= overlap_gate:\n",
    "                        overlap_counts_curr[curr_node] += 1\n",
    "            if not prelim:\n",
    "                continue\n",
    "            # classify and keep\n",
    "            for cand in prelim:\n",
    "                cand['case'] = self._classify_match_case(cand['prev_node'], cand['curr_node'], cand['features'], overlap_counts_prev, overlap_counts_curr, overlap_gate)\n",
    "            prelim = [c for c in prelim if c['case'] != 'none']\n",
    "            if not prelim:\n",
    "                continue\n",
    "            # score per case and apply gap weight\n",
    "            for cand in prelim:\n",
    "                cfg = configs[cand['case']]\n",
    "                base_sim, parts = self._weighted_similarity(cand['prev_attrs'], cand['curr_attrs'], weights=cfg.base_similarity_weights, max_dist=base_max_dist)\n",
    "                score = self._score_candidate(base_sim, parts, cand['features'], cfg)\n",
    "                gap_w = exp(-alpha * (cand['gap'] - 1)) if cand['gap'] > 1 else 1.0\n",
    "                final_score = score * gap_w\n",
    "                if final_score < cfg.similarity_threshold:\n",
    "                    continue\n",
    "                cand['base_similarity'] = float(base_sim)\n",
    "                cand['similarity_parts'] = {k: float(v) for k, v in parts.items()}\n",
    "                cand['score'] = float(score)\n",
    "                cand['gap_weight'] = float(gap_w)\n",
    "                cand['final_score'] = float(final_score)\n",
    "                all_candidates.append(cand)\n",
    "            # count per-case candidates (for diagnostics)\n",
    "            case_counts = defaultdict(int)\n",
    "            for cand in prelim:\n",
    "                case_counts[cand['case']] += 1\n",
    "            for case_name, count in case_counts.items():\n",
    "                self.last_case_counts[case_name] = self.last_case_counts.get(case_name, 0) + count\n",
    "\n",
    "    if not all_candidates:\n",
    "        return\n",
    "    # Global greedy selection by final_score\n",
    "    all_candidates.sort(key=lambda c: c['final_score'], reverse=True)\n",
    "    used_prev: Dict[Tuple[int,int], int] = defaultdict(int)\n",
    "    used_curr: Dict[Tuple[int,int], int] = defaultdict(int)\n",
    "    for cand in all_candidates:\n",
    "        u = cand['prev_node']; v = cand['curr_node']\n",
    "        cfg = configs[cand['case']]\n",
    "        if used_prev.get(u, 0) and not cfg.allow_multiple:\n",
    "            continue\n",
    "        if used_curr.get(v, 0) and not cfg.allow_multiple:\n",
    "            continue\n",
    "        self.G.add_edge(u, v,\n",
    "                        similarity=float(cand['final_score']),\n",
    "                        base_similarity=float(cand['base_similarity']),\n",
    "                        method='virtual_allpairs_enhanced',\n",
    "                        case=cand['case'],\n",
    "                        gap=int(cand['gap']), gap_weight=float(cand['gap_weight']),\n",
    "                        overlap_prev=float(cand['features']['overlap_prev']),\n",
    "                        overlap_curr=float(cand['features']['overlap_curr']),\n",
    "                        iou=float(cand['features']['iou']),\n",
    "                        centroid_distance=float(cand['features']['centroid_dist']),\n",
    "                        spatial_similarity=float(cand['similarity_parts']['spatial']),\n",
    "                        area_similarity=float(cand['similarity_parts']['area']),\n",
    "                        shape_similarity=float(cand['similarity_parts']['shape']))\n",
    "        used_prev[u] += 1\n",
    "        used_curr[v] += 1\n",
    "        self.last_selected_counts[cand['case']] = self.last_selected_counts.get(cand['case'], 0) + 1\n",
    "\n",
    "TreeTrackingGraph.build_graph_virtual_allpairs_enhanced = build_graph_virtual_allpairs_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "395fdc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph_virtual_allpairs_enhanced_cfg(self, *, base_max_dist: float = 75.0, overlap_gate: float = 0.48, min_base_similarity: float = 0.35, alpha: float = 0.9, max_gap: Optional[int] = None, case_overrides: Optional[Dict[str, Dict[str, float]]] = None) -> None:\n",
    "    \"\"\"Enhanced all-pairs with optional case threshold overrides and max_gap constraint.\"\"\"\n",
    "    if not self.crowns_gdfs:\n",
    "        self.load_data(load_images=False)\n",
    "    self.reset_graph()\n",
    "    configs = {name: replace(cfg) for name, cfg in self.case_configs.items()}\n",
    "    if case_overrides:\n",
    "        for cname, ov in case_overrides.items():\n",
    "            if cname in configs:\n",
    "                for k, v in ov.items():\n",
    "                    # only allow editing known numeric fields\n",
    "                    if hasattr(configs[cname], k):\n",
    "                        setattr(configs[cname], k, v)\n",
    "    # Add nodes\n",
    "    for om_id in self.om_ids:\n",
    "        for crown_id, row in self.crowns_gdfs[om_id].iterrows():\n",
    "            self.G.add_node((om_id, crown_id), **self.crown_attrs[om_id][crown_id])\n",
    "    self.last_case_counts = {}\n",
    "    self.last_selected_counts = {name: 0 for name in configs.keys()}\n",
    "\n",
    "    all_candidates: List[Dict[str, Any]] = []\n",
    "    for i_idx in range(len(self.om_ids)):\n",
    "        for j_idx in range(i_idx + 1, len(self.om_ids)):\n",
    "            gap = j_idx - i_idx\n",
    "            if max_gap is not None and gap > max_gap:\n",
    "                continue\n",
    "            prev_om = self.om_ids[i_idx]\n",
    "            om_id = self.om_ids[j_idx]\n",
    "            prev_nodes = [(prev_om, i) for i in range(len(self.crowns_gdfs[prev_om]))]\n",
    "            curr_nodes = [(om_id, j) for j in range(len(self.crowns_gdfs[om_id]))]\n",
    "            overlap_counts_prev: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "            overlap_counts_curr: Dict[Tuple[int, int], int] = defaultdict(int)\n",
    "            prelim: List[Dict[str, Any]] = []\n",
    "            for prev_node in prev_nodes:\n",
    "                prev_attrs = self.G.nodes[prev_node]\n",
    "                for curr_node in curr_nodes:\n",
    "                    curr_attrs = self.crown_attrs[om_id][curr_node[1]]\n",
    "                    features = self._compute_pair_metrics(prev_attrs, curr_attrs, max_dist=base_max_dist)\n",
    "                    if features['centroid_dist'] > base_max_dist:\n",
    "                        continue\n",
    "                    if features['base_similarity'] < min_base_similarity and features['iou'] < overlap_gate:\n",
    "                        continue\n",
    "                    rec = {'prev_node': prev_node, 'curr_node': curr_node, 'prev_attrs': prev_attrs, 'curr_attrs': curr_attrs, 'features': features, 'gap': gap}\n",
    "                    prelim.append(rec)\n",
    "                    if features['overlap_prev'] >= overlap_gate:\n",
    "                        overlap_counts_prev[prev_node] += 1\n",
    "                    if features['overlap_curr'] >= overlap_gate:\n",
    "                        overlap_counts_curr[curr_node] += 1\n",
    "            if not prelim:\n",
    "                continue\n",
    "            for cand in prelim:\n",
    "                cand['case'] = self._classify_match_case(cand['prev_node'], cand['curr_node'], cand['features'], overlap_counts_prev, overlap_counts_curr, overlap_gate)\n",
    "            prelim = [c for c in prelim if c['case'] != 'none']\n",
    "            if not prelim:\n",
    "                continue\n",
    "            for cand in prelim:\n",
    "                cfg = configs[cand['case']]\n",
    "                base_sim, parts = self._weighted_similarity(cand['prev_attrs'], cand['curr_attrs'], weights=cfg.base_similarity_weights, max_dist=base_max_dist)\n",
    "                score = self._score_candidate(base_sim, parts, cand['features'], cfg)\n",
    "                gap_w = exp(-alpha * (cand['gap'] - 1)) if cand['gap'] > 1 else 1.0\n",
    "                final_score = score * gap_w\n",
    "                if final_score < cfg.similarity_threshold:\n",
    "                    continue\n",
    "                cand['base_similarity'] = float(base_sim)\n",
    "                cand['similarity_parts'] = {k: float(v) for k, v in parts.items()}\n",
    "                cand['score'] = float(score)\n",
    "                cand['gap_weight'] = float(gap_w)\n",
    "                cand['final_score'] = float(final_score)\n",
    "                all_candidates.append(cand)\n",
    "            case_counts = defaultdict(int)\n",
    "            for cand in prelim:\n",
    "                case_counts[cand['case']] += 1\n",
    "            for case_name, count in case_counts.items():\n",
    "                self.last_case_counts[case_name] = self.last_case_counts.get(case_name, 0) + count\n",
    "    if not all_candidates:\n",
    "        return\n",
    "    all_candidates.sort(key=lambda c: c['final_score'], reverse=True)\n",
    "    used_prev: Dict[Tuple[int,int], int] = defaultdict(int)\n",
    "    used_curr: Dict[Tuple[int,int], int] = defaultdict(int)\n",
    "    for cand in all_candidates:\n",
    "        u = cand['prev_node']; v = cand['curr_node']\n",
    "        cfg = configs[cand['case']]\n",
    "        if used_prev.get(u, 0) and not cfg.allow_multiple:\n",
    "            continue\n",
    "        if used_curr.get(v, 0) and not cfg.allow_multiple:\n",
    "            continue\n",
    "        self.G.add_edge(u, v, similarity=float(cand['final_score']), base_similarity=float(cand['base_similarity']), method='virtual_allpairs_enhanced_cfg', case=cand['case'], gap=int(cand['gap']), gap_weight=float(cand['gap_weight']), overlap_prev=float(cand['features']['overlap_prev']), overlap_curr=float(cand['features']['overlap_curr']), iou=float(cand['features']['iou']), centroid_distance=float(cand['features']['centroid_dist']), spatial_similarity=float(cand['similarity_parts']['spatial']), area_similarity=float(cand['similarity_parts']['area']), shape_similarity=float(cand['similarity_parts']['shape']))\n",
    "        used_prev[u] += 1\n",
    "        used_curr[v] += 1\n",
    "        self.last_selected_counts[cand['case']] = self.last_selected_counts.get(cand['case'], 0) + 1\n",
    "\n",
    "TreeTrackingGraph.build_graph_virtual_allpairs_enhanced_cfg = build_graph_virtual_allpairs_enhanced_cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "496bec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detailed_matching_metrics(tracker: TreeTrackingGraph) -> Dict[str, Any]:\n",
    "    G = tracker.G\n",
    "    om_ids = tracker.om_ids\n",
    "    out: Dict[str, Any] = {}\n",
    "\n",
    "    # Basic counts\n",
    "    out['num_nodes'] = G.number_of_nodes()\n",
    "    out['num_edges'] = G.number_of_edges()\n",
    "\n",
    "    # Edges by case and by gap\n",
    "    by_case: Dict[str, int] = defaultdict(int)\n",
    "    by_gap: Dict[int, int] = defaultdict(int)\n",
    "    by_case_gap: Dict[str, Dict[int, int]] = defaultdict(lambda: defaultdict(int))\n",
    "    sim_values: List[float] = []\n",
    "    iou_values: List[float] = []\n",
    "    overlap_prev_values: List[float] = []\n",
    "    overlap_curr_values: List[float] = []\n",
    "    centroid_values: List[float] = []\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        case = d.get('case', 'unknown')\n",
    "        by_case[case] += 1\n",
    "        gap = int(d.get('gap', (v[0] - u[0] if isinstance(v[0], int) and isinstance(u[0], int) else 1)))\n",
    "        by_gap[gap] += 1\n",
    "        by_case_gap[case][gap] += 1\n",
    "        sim_values.append(float(d.get('similarity', 0.0)))\n",
    "        iou_values.append(float(d.get('iou', 0.0)))\n",
    "        overlap_prev_values.append(float(d.get('overlap_prev', 0.0)))\n",
    "        overlap_curr_values.append(float(d.get('overlap_curr', 0.0)))\n",
    "        centroid_values.append(float(d.get('centroid_distance', 0.0)))\n",
    "    out['edges_by_case'] = dict(sorted(by_case.items(), key=lambda kv: (-kv[1], kv[0])))\n",
    "    out['edges_by_gap'] = dict(sorted(by_gap.items()))\n",
    "    out['edges_by_case_and_gap'] = {k: dict(sorted(v.items())) for k, v in by_case_gap.items()}\n",
    "\n",
    "    # Distributions\n",
    "    def _stats(vals: List[float]) -> Dict[str, float]:\n",
    "        if not vals:\n",
    "            return {'count': 0, 'mean': 0.0, 'median': 0.0, 'min': 0.0, 'max': 0.0}\n",
    "        return {\n",
    "            'count': len(vals),\n",
    "            'mean': float(np.mean(vals)),\n",
    "            'median': float(np.median(vals)),\n",
    "            'min': float(np.min(vals)),\n",
    "            'max': float(np.max(vals)),\n",
    "        }\n",
    "    out['similarity_stats'] = _stats(sim_values)\n",
    "    out['iou_stats'] = _stats(iou_values)\n",
    "    out['overlap_prev_stats'] = _stats(overlap_prev_values)\n",
    "    out['overlap_curr_stats'] = _stats(overlap_curr_values)\n",
    "    out['centroid_distance_stats'] = _stats(centroid_values)\n",
    "\n",
    "    # Per consecutive OM pair\n",
    "    pair_stats: Dict[str, Dict[str, float]] = {}\n",
    "    for i in range(len(om_ids) - 1):\n",
    "        om1, om2 = om_ids[i], om_ids[i + 1]\n",
    "        om1_nodes = [n for n in G.nodes if n[0] == om1]\n",
    "        om2_nodes = [n for n in G.nodes if n[0] == om2]\n",
    "        matches = sum(1 for u, v in G.edges() if u[0] == om1 and v[0] == om2)\n",
    "        possible = min(len(om1_nodes), len(om2_nodes))\n",
    "        rate = matches / possible if possible > 0 else 0.0\n",
    "        pair_stats[f\"{om1}->{om2}\"] = {\n",
    "            'matches': matches,\n",
    "            'possible': possible,\n",
    "            'rate': float(rate),\n",
    "        }\n",
    "    out['pair_stats'] = pair_stats\n",
    "\n",
    "    # Chains\n",
    "    chains = tracker._extract_all_chains()\n",
    "    lengths = [len(c) for c in chains]\n",
    "    out['chain_count'] = len(chains)\n",
    "    out['chain_length_stats'] = _stats([float(l) for l in lengths])\n",
    "    out['long_chains'] = [c for c in chains if len(c) >= max(2, int(np.percentile(lengths, 90)))]\n",
    "\n",
    "    # Degrees\n",
    "    out['out_degree_distribution'] = tracker.graph_complexity_metrics()['out_degree_distribution']\n",
    "    out['in_degree_distribution'] = tracker.graph_complexity_metrics()['in_degree_distribution']\n",
    "\n",
    "    # High-confidence chains\n",
    "    def _edge_info(tr, ch):\n",
    "        for u, v in zip(ch, ch[1:]):\n",
    "            yield tr.G.get_edge_data(u, v) or {}\n",
    "    hc = []\n",
    "    for ch in chains:\n",
    "        if len(ch) < 2:\n",
    "            continue\n",
    "        edges = list(_edge_info(tracker, ch))\n",
    "        if edges and all((e.get('case') in {'one_to_one','containment'}) and (e.get('similarity',0.0) >= 0.8) for e in edges):\n",
    "            hc.append(ch)\n",
    "    out['high_conf_chain_count'] = len(hc)\n",
    "    out['high_conf_avg_length'] = float(np.mean([len(c) for c in hc])) if hc else 0.0\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784b18e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Enhanced Virtual All-Pairs ===\n",
      "# Tree Tracking Quality Assessment Report\n",
      "Total Trees Detected: 626\n",
      "Total Tracking Edges: 2\n",
      "Overall Match Rate: 0.004\n",
      "Average Chain Length: 1.00\n",
      "Maximum Chain Length: 2\n",
      "Match Rates by Orthomosaic Pair:\n",
      "- 1->2: 1/80 (0.013)\n",
      "- 2->3: 0/116 (0.000)\n",
      "- 3->4: 0/130 (0.000)\n",
      "- 4->5: 1/150 (0.007)\n",
      "\n",
      "Chain Length Distribution:\n",
      "- Length 1: 622 trees\n",
      "- Length 2: 2 trees\n",
      "\n",
      "Edge selection by case:\n",
      "- one_to_one: 2 / 3 (0.67)\n",
      "- containment: 0 / 20 (0.00)\n",
      "---\n",
      "# Graph Complexity Report\n",
      "Nodes: 626\n",
      "Edges: 2\n",
      "Avg out-degree: 0.003\n",
      "Avg in-degree: 0.003\n",
      "Zero out-degree nodes: 624\n",
      "Zero in-degree nodes: 624\n",
      "Weakly connected components: 624 (sizes head: [2, 2, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Strongly connected components: 626 (sizes head: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Average diameter: 0.003\n",
      "Median diameter: 0.000\n",
      "Max diameter: 1\n",
      "---\n",
      "Edges by case: {'one_to_one': 2}\n",
      "Edges by gap: {1: 2}\n",
      "Similarity stats: {'count': 2, 'mean': 0.8355917390249512, 'median': 0.8355917390249512, 'min': 0.8321772990370863, 'max': 0.839006179012816}\n",
      "IoU stats: {'count': 2, 'mean': 0.6738917296458065, 'median': 0.6738917296458065, 'min': 0.650735109053495, 'max': 0.697048350238118}\n",
      "Overlap(prev) stats: {'count': 2, 'mean': 0.8497302002706046, 'median': 0.8497302002706046, 'min': 0.7669538171937105, 'max': 0.9325065833474987}\n",
      "Overlap(curr) stats: {'count': 2, 'mean': 0.7726016145015054, 'median': 0.7726016145015054, 'min': 0.7340838383942863, 'max': 0.8111193906087246}\n",
      "Centroid distance stats: {'count': 2, 'mean': 0.8577147656422424, 'median': 0.8577147656422424, 'min': 0.8175072679223973, 'max': 0.8979222633620876}\n",
      "High-confidence chains: 2 avg len: 2.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'../../output/virtual_allpairs_enhanced_15oct_detailed_matching_metrics.json'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run enhanced virtual-all-pairs and print/save metrics\n",
    "\n",
    "enhanced = TreeTrackingGraph()\n",
    "enhanced.load_data(load_images=False)\n",
    "enhanced.build_graph_virtual_allpairs_enhanced(base_max_dist=75.0, overlap_gate=0.48, min_base_similarity=0.35, alpha=0.9)\n",
    "\n",
    "# Core reports\n",
    "q_report, q = enhanced.quality_report()\n",
    "c_report, c = enhanced.complexity_report()\n",
    "\n",
    "# Detailed metrics\n",
    "metrics = compute_detailed_matching_metrics(enhanced)\n",
    "\n",
    "print(\"=== Enhanced Virtual All-Pairs ===\")\n",
    "print(q_report)\n",
    "print(\"---\")\n",
    "print(c_report)\n",
    "print(\"---\")\n",
    "print(\"Edges by case:\", metrics['edges_by_case'])\n",
    "print(\"Edges by gap:\", metrics['edges_by_gap'])\n",
    "print(\"Similarity stats:\", metrics['similarity_stats'])\n",
    "print(\"IoU stats:\", metrics['iou_stats'])\n",
    "print(\"Overlap(prev) stats:\", metrics['overlap_prev_stats'])\n",
    "print(\"Overlap(curr) stats:\", metrics['overlap_curr_stats'])\n",
    "print(\"Centroid distance stats:\", metrics['centroid_distance_stats'])\n",
    "print(\"High-confidence chains:\", metrics['high_conf_chain_count'], \"avg len:\", f\"{metrics['high_conf_avg_length']:.2f}\")\n",
    "\n",
    "# Save\n",
    "prefix = 'virtual_allpairs_enhanced_15oct'\n",
    "enhanced.save_text(q_report, f'{prefix}_quality_report.txt')\n",
    "enhanced.save_json(q, f'{prefix}_quality_metrics.json')\n",
    "enhanced.save_text(c_report, f'{prefix}_complexity_report.txt')\n",
    "enhanced.save_json(c, f'{prefix}_complexity_metrics.json')\n",
    "enhanced.save_json(metrics, f'{prefix}_detailed_matching_metrics.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0284092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tried alpha=0.5, dist=75.0, og=0.45, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=75.0, og=0.45, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=75.0, og=0.45, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=75.0, og=0.48, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=75.0, og=0.48, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=75.0, og=0.48, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=75.0, og=0.52, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=75.0, og=0.52, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=75.0, og=0.52, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=85.0, og=0.45, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=85.0, og=0.45, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=85.0, og=0.45, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=85.0, og=0.48, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=85.0, og=0.48, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=85.0, og=0.48, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=85.0, og=0.52, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=85.0, og=0.52, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=85.0, og=0.52, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=95.0, og=0.45, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=95.0, og=0.45, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=95.0, og=0.45, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=95.0, og=0.48, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=95.0, og=0.48, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=95.0, og=0.48, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=95.0, og=0.52, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=95.0, og=0.52, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.5, dist=95.0, og=0.52, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=75.0, og=0.45, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=75.0, og=0.45, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=75.0, og=0.45, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=75.0, og=0.48, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=75.0, og=0.48, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=75.0, og=0.48, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=75.0, og=0.52, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=75.0, og=0.52, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=75.0, og=0.52, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=85.0, og=0.45, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=85.0, og=0.45, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=85.0, og=0.45, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=85.0, og=0.48, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=85.0, og=0.48, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=85.0, og=0.48, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=85.0, og=0.52, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=85.0, og=0.52, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=85.0, og=0.52, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=95.0, og=0.45, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=95.0, og=0.45, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=95.0, og=0.45, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=95.0, og=0.48, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=95.0, og=0.48, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=95.0, og=0.48, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=95.0, og=0.52, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=95.0, og=0.52, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=0.8, dist=95.0, og=0.52, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=75.0, og=0.45, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=75.0, og=0.45, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=75.0, og=0.45, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=75.0, og=0.48, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=75.0, og=0.48, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=75.0, og=0.48, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=75.0, og=0.52, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=75.0, og=0.52, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=75.0, og=0.52, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=85.0, og=0.45, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=85.0, og=0.45, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=85.0, og=0.45, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=85.0, og=0.48, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=85.0, og=0.48, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=85.0, og=0.48, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=85.0, og=0.52, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=85.0, og=0.52, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=85.0, og=0.52, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=95.0, og=0.45, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=95.0, og=0.45, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=95.0, og=0.45, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=95.0, og=0.48, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=95.0, og=0.48, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=95.0, og=0.48, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=95.0, og=0.52, thrÎ”=-0.05 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=95.0, og=0.52, thrÎ”=-0.02 -> edges=2, hc=2, rate=0.004\n",
      "Tried alpha=1.0, dist=95.0, og=0.52, thrÎ”=0.0 -> edges=2, hc=2, rate=0.004\n",
      "\n",
      "Best params: {'alpha': 0.5, 'base_max_dist': 75.0, 'overlap_gate': 0.45, 'thr_delta': -0.05, 'max_gap': 2}\n",
      "Score: 3.380201680672269\n",
      "# Tree Tracking Quality Assessment Report\n",
      "Total Trees Detected: 626\n",
      "Total Tracking Edges: 2\n",
      "Overall Match Rate: 0.004\n",
      "Average Chain Length: 1.00\n",
      "Maximum Chain Length: 2\n",
      "Match Rates by Orthomosaic Pair:\n",
      "- 1->2: 1/80 (0.013)\n",
      "- 2->3: 0/116 (0.000)\n",
      "- 3->4: 0/130 (0.000)\n",
      "- 4->5: 1/150 (0.007)\n",
      "\n",
      "Chain Length Distribution:\n",
      "- Length 1: 622 trees\n",
      "- Length 2: 2 trees\n",
      "\n",
      "Edge selection by case:\n",
      "- one_to_one: 2 / 2 (1.00)\n",
      "- containment: 0 / 10 (0.00)\n",
      "---\n",
      "# Graph Complexity Report\n",
      "Nodes: 626\n",
      "Edges: 2\n",
      "Avg out-degree: 0.003\n",
      "Avg in-degree: 0.003\n",
      "Zero out-degree nodes: 624\n",
      "Zero in-degree nodes: 624\n",
      "Weakly connected components: 624 (sizes head: [2, 2, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Strongly connected components: 626 (sizes head: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
      "Average diameter: 0.003\n",
      "Median diameter: 0.000\n",
      "Max diameter: 1\n"
     ]
    }
   ],
   "source": [
    "# Parameter sweep over enhanced virtual edges to improve recall while preserving precision\n",
    "from itertools import product\n",
    "\n",
    "def run_variant(alpha, base_max_dist, overlap_gate, thr_delta, max_gap=2):\n",
    "    cfg = {'one_to_one': {'similarity_threshold': TreeTrackingGraph().case_configs['one_to_one'].similarity_threshold + thr_delta}, 'containment': {'similarity_threshold': TreeTrackingGraph().case_configs['containment'].similarity_threshold + thr_delta}}\n",
    "    t = TreeTrackingGraph()\n",
    "    t.load_data(load_images=False)\n",
    "    t.build_graph_virtual_allpairs_enhanced_cfg(base_max_dist=base_max_dist, overlap_gate=overlap_gate, min_base_similarity=0.35, alpha=alpha, max_gap=max_gap, case_overrides=cfg)\n",
    "    q_report, q = t.quality_report()\n",
    "    c_report, c = t.complexity_report()\n",
    "    m = compute_detailed_matching_metrics(t)\n",
    "    # composite score: prioritize high-conf chains, then match rate, penalize zero-degree nodes\n",
    "    composite = (m['high_conf_chain_count'] * 2.0) + q['overall_match_rate'] - 0.0005 * (c.get('zero_in_degree_nodes',0) + c.get('zero_out_degree_nodes',0))\n",
    "    return {'t': t, 'q': q, 'c': c, 'm': m, 'q_report': q_report, 'c_report': c_report, 'params': {'alpha': alpha, 'base_max_dist': base_max_dist, 'overlap_gate': overlap_gate, 'thr_delta': thr_delta, 'max_gap': max_gap}, 'score': composite}\n",
    "\n",
    "alphas = [0.5, 0.8, 1.0]\n",
    "base_dists = [75.0, 85.0, 95.0]\n",
    "overgates = [0.45, 0.48, 0.52]\n",
    "thr_deltas = [-0.05, -0.02, 0.0]\n",
    "\n",
    "results = []\n",
    "for a, d, og, td in product(alphas, base_dists, overgates, thr_deltas):\n",
    "    res = run_variant(a, d, og, td, max_gap=2)\n",
    "    results.append(res)\n",
    "    print(f\"Tried alpha={a}, dist={d}, og={og}, thrÎ”={td} -> edges={res['c']['num_edges']}, hc={res['m']['high_conf_chain_count']}, rate={res['q']['overall_match_rate']:.3f}\")\n",
    "\n",
    "# pick best\n",
    "best = max(results, key=lambda r: r['score']) if results else None\n",
    "if best:\n",
    "    print(\"\\nBest params:\", best['params'])\n",
    "    print(\"Score:\", best['score'])\n",
    "    print(best['q_report'])\n",
    "    print('---')\n",
    "    print(best['c_report'])\n",
    "    # save artifacts\n",
    "    prefix = 'virtual_allpairs_enhanced_sweep_best'\n",
    "    best['t'].save_text(best['q_report'], f'{prefix}_quality_report.txt')\n",
    "    best['t'].save_json(best['q'], f'{prefix}_quality_metrics.json')\n",
    "    best['t'].save_text(best['c_report'], f'{prefix}_complexity_report.txt')\n",
    "    best['t'].save_json(best['c'], f'{prefix}_complexity_metrics.json')\n",
    "    best['t'].save_json(best['m'], f'{prefix}_detailed_matching_metrics.json')\n",
    "else:\n",
    "    print('No results found in sweep.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02e92995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization helpers: overlay matched pairs across OMs for top chains\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def visualize_chains(tracker: TreeTrackingGraph, chains: List[List[Tuple[int,int]]], k: int = 5, prefix: str = 'virtual_vis'):\n",
    "    os.makedirs(tracker.output_dir, exist_ok=True)\n",
    "    shown = 0\n",
    "    for idx, ch in enumerate(sorted(chains, key=lambda c: len(c), reverse=True)):\n",
    "        if shown >= k:\n",
    "            break\n",
    "        # build a simple plot: per edge, plot prev and curr polygons in different colors\n",
    "        fig, axes = plt.subplots(1, len(ch)-1 if len(ch)>1 else 1, figsize=(4*(len(ch)-1 if len(ch)>1 else 1), 4))\n",
    "        if not isinstance(axes, np.ndarray):\n",
    "            axes = np.array([axes])\n",
    "        for ax, (u, v) in zip(axes, zip(ch, ch[1:])):\n",
    "            prev_geom = tracker.G.nodes[u]['geometry']\n",
    "            curr_geom = tracker.G.nodes[v]['geometry']\n",
    "            x,y = prev_geom.exterior.xy if hasattr(prev_geom, 'exterior') else ([],[])\n",
    "            ax.plot(x, y, color='tab:blue', label=f'{u[0]}:{u[1]}')\n",
    "            x2,y2 = curr_geom.exterior.xy if hasattr(curr_geom, 'exterior') else ([],[])\n",
    "            ax.plot(x2, y2, color='tab:orange', label=f'{v[0]}:{v[1]}')\n",
    "            e = tracker.G.get_edge_data(u,v) or {}\n",
    "            ax.set_title(f\"{u[0]}â†’{v[0]} | sim={e.get('similarity',0):.2f} iou={e.get('iou',0):.2f}\")\n",
    "            ax.legend(loc='best', fontsize=8)\n",
    "            ax.set_aspect('equal', 'box')\n",
    "        plt.tight_layout()\n",
    "        out_path = os.path.join(tracker.output_dir, f\"{prefix}_chain_{idx+1}.png\")\n",
    "        plt.savefig(out_path, dpi=150)\n",
    "        plt.close(fig)\n",
    "        shown += 1\n",
    "    return shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fbad827f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Best] params: {'alpha': 0.5, 'base_max_dist': 75.0, 'overlap_gate': 0.45, 'thr_delta': -0.05, 'max_gap': 2}\n",
      "[Best] edges: 2 high-conf chains: 2\n",
      "Saved 2 chain visualizations to ../../output\n"
     ]
    }
   ],
   "source": [
    "# Run sweep and visualize best\n",
    "best_result = None\n",
    "try:\n",
    "    # If sweep already ran in this session, reuse variable 'best'; else run now\n",
    "    best_result = best  # noqa: F821\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "if best_result is None:\n",
    "    # Execute the sweep cell above programmatically by reusing definitions\n",
    "    from itertools import product\n",
    "    alphas = [0.5, 0.8, 1.0]\n",
    "    base_dists = [75.0, 85.0, 95.0]\n",
    "    overgates = [0.45, 0.48, 0.52]\n",
    "    thr_deltas = [-0.05, -0.02, 0.0]\n",
    "    results = []\n",
    "    for a, d, og, td in product(alphas, base_dists, overgates, thr_deltas):\n",
    "        res = run_variant(a, d, og, td, max_gap=2)\n",
    "        results.append(res)\n",
    "        print(f\"Tried alpha={a}, dist={d}, og={og}, thrÎ”={td} -> edges={res['c']['num_edges']}, hc={res['m']['high_conf_chain_count']}, rate={res['q']['overall_match_rate']:.3f}\")\n",
    "    best_result = max(results, key=lambda r: r['score']) if results else None\n",
    "\n",
    "if best_result:\n",
    "    print(\"\\n[Best] params:\", best_result['params'])\n",
    "    print(\"[Best] edges:\", best_result['c']['num_edges'], \"high-conf chains:\", best_result['m']['high_conf_chain_count'])\n",
    "    # visualize top chains\n",
    "    chains = best_result['t']._extract_all_chains()\n",
    "    # pick high-confidence chains first if available\n",
    "    hc = []\n",
    "    def _edge_info(tr, ch):\n",
    "        for u, v in zip(ch, ch[1:]):\n",
    "            yield tr.G.get_edge_data(u, v) or {}\n",
    "    for ch in chains:\n",
    "        if len(ch) < 2: continue\n",
    "        edges = list(_edge_info(best_result['t'], ch))\n",
    "        if edges and all((e.get('case') in {'one_to_one','containment'}) and (e.get('similarity',0.0) >= 0.8) for e in edges):\n",
    "            hc.append(ch)\n",
    "    selected = hc if hc else [c for c in chains if len(c) >= 2]\n",
    "    n_plotted = visualize_chains(best_result['t'], selected, k=6, prefix='virtual_allpairs_enhanced_best')\n",
    "    print(f\"Saved {n_plotted} chain visualizations to {best_result['t'].output_dir}\")\n",
    "else:\n",
    "    print('No best result available to visualize.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d68f1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Shuffle aggregation ===\n",
      "Edges: 0 | Match rate: 0.000\n",
      "Avg chain: 1.00 | Max chain: 1\n",
      "High-conf chains: 0 (avg len 0.00)\n",
      "Zero in/out: 626/626 | Max diameter: 0\n",
      "\n",
      "=== Virtual all-pairs ===\n",
      "Edges: 3 | Match rate: 0.004\n",
      "Avg chain: 1.00 | Max chain: 2\n",
      "High-conf chains: 3 (avg len 2.00)\n",
      "Zero in/out: 623/623 | Max diameter: 1\n",
      "\n",
      ">>> Preferred: Virtual all-pairs â€” better connectivity or confidence under current data.\n"
     ]
    }
   ],
   "source": [
    "# Run: shuffled-order aggregation vs virtual all-pairs; compare metrics\n",
    "from copy import deepcopy\n",
    "import math\n",
    "\n",
    "def run_and_metrics(build_fn):\n",
    "    tracker = TreeTrackingGraph()\n",
    "    # Reuse loaded data for speed\n",
    "    tracker.load_data(load_images=False)\n",
    "    build_fn(tracker)\n",
    "    q_report, q_metrics = tracker.quality_report()\n",
    "    c_report, c_metrics = tracker.complexity_report()\n",
    "    # High-confidence chains (strict cases, high similarity)\n",
    "    def _edge_info(tr, ch):\n",
    "        for u, v in zip(ch, ch[1:]):\n",
    "            yield tr.G.get_edge_data(u, v) or {}\n",
    "    chains = tracker._extract_all_chains()\n",
    "    hc = []\n",
    "    for ch in chains:\n",
    "        if len(ch) < 2:\n",
    "            continue\n",
    "        edges = list(_edge_info(tracker, ch))\n",
    "        if edges and all((e.get('case') in {'one_to_one','containment'}) and (e.get('similarity',0.0) >= 0.8) for e in edges):\n",
    "            hc.append(ch)\n",
    "    return {\n",
    "        'tracker': tracker,\n",
    "        'q_report': q_report,\n",
    "        'q': q_metrics,\n",
    "        'c_report': c_report,\n",
    "        'c': c_metrics,\n",
    "        'hc_count': len(hc),\n",
    "        'hc_avg_len': float(np.mean([len(ch) for ch in hc])) if hc else 0.0,\n",
    "    }\n",
    "\n",
    "# Strategy A: shuffle aggregation\n",
    "def build_shuffle(tracker: TreeTrackingGraph):\n",
    "    tracker.build_graph_from_shuffles(num_shuffles=8, seed=42, base_max_dist=75.0, overlap_gate=0.48, min_base_similarity=0.35, min_frequency=0.5)\n",
    "\n",
    "# Strategy B: virtual all-pairs\n",
    "def build_virtual(tracker: TreeTrackingGraph):\n",
    "    tracker.build_graph_virtual_allpairs(base_max_dist=75.0, overlap_gate=0.48, min_base_similarity=0.35)\n",
    "\n",
    "shuffle_out = run_and_metrics(build_shuffle)\n",
    "virtual_out = run_and_metrics(build_virtual)\n",
    "\n",
    "def summarize(name, out):\n",
    "    q, c = out['q'], out['c']\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(f\"Edges: {c['num_edges']} | Match rate: {q['overall_match_rate']:.3f}\")\n",
    "    print(f\"Avg chain: {q.get('average_chain_length',0):.2f} | Max chain: {q.get('max_chain_length',0)}\")\n",
    "    print(f\"High-conf chains: {out['hc_count']} (avg len {out['hc_avg_len']:.2f})\")\n",
    "    print(f\"Zero in/out: {c.get('zero_in_degree_nodes',0)}/{c.get('zero_out_degree_nodes',0)} | Max diameter: {c.get('max_diameter',0)}\")\n",
    "\n",
    "summarize('Shuffle aggregation', shuffle_out)\n",
    "summarize('Virtual all-pairs', virtual_out)\n",
    "\n",
    "# Simple decision heuristic: prefer higher high-conf chains, then higher match rate, then fewer zero-degree nodes\n",
    "def decide(a, b):\n",
    "    ak = (a['hc_count'], a['q']['overall_match_rate'], -a['c'].get('zero_in_degree_nodes',0)-a['c'].get('zero_out_degree_nodes',0))\n",
    "    bk = (b['hc_count'], b['q']['overall_match_rate'], -b['c'].get('zero_in_degree_nodes',0)-b['c'].get('zero_out_degree_nodes',0))\n",
    "    return 'A' if ak > bk else ('B' if bk > ak else 'tie')\n",
    "\n",
    "choice = decide(shuffle_out, virtual_out)\n",
    "if choice == 'A':\n",
    "    print(\"\\n>>> Preferred: Shuffle aggregation â€” more consistent high-confidence chains / match quality.\")\n",
    "elif choice == 'B':\n",
    "    print(\"\\n>>> Preferred: Virtual all-pairs â€” better connectivity or confidence under current data.\")\n",
    "else:\n",
    "    print(\"\\n>>> Strategies are comparable on current metrics.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
